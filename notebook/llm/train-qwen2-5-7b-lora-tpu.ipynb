{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T02:05:21.524159Z",
     "iopub.status.busy": "2025-04-17T02:05:21.523945Z",
     "iopub.status.idle": "2025-04-17T02:07:46.995688Z",
     "shell.execute_reply": "2025-04-17T02:07:46.994211Z",
     "shell.execute_reply.started": "2025-04-17T02:05:21.524138Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q optimum-tpu -f https://storage.googleapis.com/libtpu-releases/index.html\n",
    "!pip install -q -e . -f https://storage.googleapis.com/libtpu-releases/index.html\n",
    "!pip install -q trl peft\n",
    "!pip install -q ipywidgets widgetsnbextension\n",
    "!pip install -q datasets evaluate accelerate\n",
    "!pip install -q nltk jieba evaluate rouge_score sacrebleu\n",
    "\n",
    "!export PJRT_DEVICE=TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T02:07:46.997473Z",
     "iopub.status.busy": "2025-04-17T02:07:46.997177Z",
     "iopub.status.idle": "2025-04-17T02:08:41.453534Z",
     "shell.execute_reply": "2025-04-17T02:08:41.452379Z",
     "shell.execute_reply.started": "2025-04-17T02:07:46.997446Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM,  AutoTokenizer, DataCollatorForLanguageModeling\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from peft import LoraConfig, get_peft_model, get_peft_model_state_dict\n",
    "from optimum.tpu import fsdp_v2\n",
    "import os\n",
    "from transformers import TrainerCallback\n",
    "import torch_xla.core.xla_model as xm\n",
    "import json\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "import jieba\n",
    "import numpy as np\n",
    "from accelerate.utils import extract_model_from_parallel\n",
    "\n",
    "os.environ[\"TPU_NAME\"] = \"v3-8\"\n",
    "try:\n",
    "    os.environ.pop('TPU_PROCESS_ADDRESSES')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T02:08:41.455171Z",
     "iopub.status.busy": "2025-04-17T02:08:41.454625Z",
     "iopub.status.idle": "2025-04-17T02:08:41.461026Z",
     "shell.execute_reply": "2025-04-17T02:08:41.460044Z",
     "shell.execute_reply.started": "2025-04-17T02:08:41.455142Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_corpus(file_path):\n",
    "    import random\n",
    "    random.seed(42)\n",
    "    \"\"\"加载语料\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    def wrap_prompt(src, tgt, lang):\n",
    "        # 构建统一的系统提示\n",
    "        system_prompt = f\"\"\"<|im_start|>system\n",
    "你是专注于xxx领域的资深翻译专家，专门负责将英文文档精准翻译成 {lang}。\n",
    "\n",
    "## 翻译原则：\n",
    "\n",
    "### 1. 术语准确性\n",
    "- 严格使用xxx行业标准术语\n",
    "- 保持技术参数的精确性和专业性\n",
    "- 遵循xxx领域的权威表达方式\n",
    "\n",
    "### 2. 句式结构与技术逻辑忠实性\n",
    "- 英文常见被动语态，在{lang}中需转换为符合习惯的**主动语态或自然表述**，避免生硬直译\n",
    "- 对复杂英文长句，按{lang}习惯进行合理拆分或重组，确保**逻辑清晰、易于理解**，同时**不丢失任何细节**\n",
    "- 逻确保技术逻辑连接词翻译准确，因果关系、条件关系明确\n",
    "\n",
    "**请逐句审阅，严格遵循以上所有规范。**<|im_end|>\"\"\"\n",
    "\n",
    "        user_prompt = f\"<|im_start|>user\\n### 请将以下文本准确翻译成{lang}：\\n{src}<|im_end|>\\n\\n\"\n",
    "\n",
    "        # 构建助手回复\n",
    "        assistant_response = f\"{tgt}<|im_end|>\"\n",
    "\n",
    "        return {\n",
    "                \"systemprompt\": system_prompt,\n",
    "                \"userprompt\": user_prompt,\n",
    "                \"prompt\": f\"{system_prompt}\\n{user_prompt}\",\n",
    "                \"completion\": assistant_response,\n",
    "            }\n",
    "\n",
    "    def process_pairs(pairs, shuffle=True):\n",
    "        # 构建ChatML格式的数据，按语言类型分组\n",
    "        zhcn_samples = []\n",
    "        zhtw_samples = []\n",
    "\n",
    "        for src, tgt, *tgt2 in pairs:\n",
    "            zhcn_samples.append(wrap_prompt(src, tgt, \"简体中文\"))\n",
    "            if len(tgt2) > 0:\n",
    "                zhtw_samples.append(wrap_prompt(src, tgt2[0], \"繁体中文\"))\n",
    "\n",
    "        # 按语言类型分组，避免在同一序列中混合不同语言\n",
    "        processed = []\n",
    "        if shuffle:\n",
    "            random.shuffle(zhcn_samples)\n",
    "            random.shuffle(zhtw_samples)\n",
    "        # 先添加简体中文样本\n",
    "        processed.extend(zhcn_samples)\n",
    "        # 再添加繁体中文样本\n",
    "        processed.extend(zhtw_samples)\n",
    "\n",
    "        return processed\n",
    "    \n",
    "    train_data = process_pairs(data['train'])\n",
    "    val_data = process_pairs(data['validation'],False)\n",
    "    \n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T02:08:41.462346Z",
     "iopub.status.busy": "2025-04-17T02:08:41.462091Z",
     "iopub.status.idle": "2025-04-17T02:08:41.491506Z",
     "shell.execute_reply": "2025-04-17T02:08:41.490383Z",
     "shell.execute_reply.started": "2025-04-17T02:08:41.462322Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def pack_sequences(examples, tokenizer, max_length=768):\n",
    "    import random\n",
    "    random.seed(42)\n",
    "\n",
    "    # 第一步：遍历所有语料，编码并按语言分组\n",
    "    chs_prompts = []  # 简体中文prompts\n",
    "    cht_prompts = []  # 繁体中文prompts\n",
    "    chs_full_system_prompt = None  # 简体中文完整system prompt\n",
    "    cht_full_system_prompt = None  # 繁体中文完整system prompt\n",
    "    chs_short_system_prompt = None  # 简体中文简短system prompt\n",
    "    cht_short_system_prompt = None  # 繁体中文简短system prompt\n",
    "\n",
    "    def extract_target_language(user_prompt):\n",
    "        if \"简体中文\" in user_prompt:\n",
    "            return \"简体中文\"\n",
    "        else:\n",
    "            return \"繁体中文\"\n",
    "\n",
    "    def create_short_system_prompt(language):\n",
    "        \"\"\"创建简短版本的系统提示词\"\"\"\n",
    "        if language == \"简体中文\":\n",
    "            return f\"\\n<|im_start|>system\\n你是专注于xxx领域的资深翻译专家，专门负责将英文文档精准翻译成简体中文。<|im_end|>\"\n",
    "        else:\n",
    "            return f\"\\n<|im_start|>system\\n你是专注于xxx领域的资深翻译专家，专门负责将英文文档精准翻译成繁体中文。<|im_end|>\"\n",
    "\n",
    "    # 遍历所有语料，编码user prompt、completion，并拼接为完整格式\n",
    "    for example in examples:\n",
    "        target_language = extract_target_language(example[\"userprompt\"])\n",
    "\n",
    "        # 编码user prompt和completion\n",
    "        user_prompt_tokens = tokenizer(\"\\n\" + example[\"userprompt\"],\n",
    "                                     add_special_tokens=False,\n",
    "                                     return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        completion_tokens = tokenizer(example[\"completion\"],\n",
    "                                    add_special_tokens=False,\n",
    "                                    return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "\n",
    "        # 拼接为完整的prompt_tokens（不包含system prompt）\n",
    "        prompt_tokens = torch.cat([user_prompt_tokens, completion_tokens])\n",
    "\n",
    "        # 按语言保存到对应数组\n",
    "        if target_language == \"简体中文\":\n",
    "            chs_prompts.append(prompt_tokens.tolist())\n",
    "            # 保存完整和简短system prompt（只需要保存一次）\n",
    "            if chs_full_system_prompt is None:\n",
    "                chs_full_system_prompt = tokenizer(example[\"systemprompt\"],\n",
    "                                                 add_special_tokens=False,\n",
    "                                                 return_tensors=\"pt\")[\"input_ids\"][0].tolist()\n",
    "                # 创建并编码简短版本\n",
    "                short_prompt = create_short_system_prompt(target_language)\n",
    "                chs_short_system_prompt = tokenizer(short_prompt,\n",
    "                                                  add_special_tokens=False,\n",
    "                                                  return_tensors=\"pt\")[\"input_ids\"][0].tolist()\n",
    "        else:\n",
    "            cht_prompts.append(prompt_tokens.tolist())\n",
    "            # 保存完整和简短system prompt（只需要保存一次）\n",
    "            if cht_full_system_prompt is None:\n",
    "                cht_full_system_prompt = tokenizer(example[\"systemprompt\"],\n",
    "                                                 add_special_tokens=False,\n",
    "                                                 return_tensors=\"pt\")[\"input_ids\"][0].tolist()\n",
    "                # 创建并编码简短版本\n",
    "                short_prompt = create_short_system_prompt(target_language)\n",
    "                cht_short_system_prompt = tokenizer(short_prompt,\n",
    "                                                  add_special_tokens=False,\n",
    "                                                  return_tensors=\"pt\")[\"input_ids\"][0].tolist()\n",
    "\n",
    "    # 第二步：按元素长度进行升序排序\n",
    "    sorted_chs_prompts = sorted(chs_prompts, key=len)\n",
    "    sorted_cht_prompts = sorted(cht_prompts, key=len)\n",
    "\n",
    "    # 第三步：按当前逻辑遍历原始prompts，合并对应的system prompt和翻译prompt\n",
    "    packed_inputs = []\n",
    "    current_tokens = []\n",
    "    current_language = None\n",
    "\n",
    "    def get_system_prompts_for_language(language):\n",
    "        \"\"\"获取指定语言的完整和简短系统提示词\"\"\"\n",
    "        if language == \"简体中文\":\n",
    "            return chs_full_system_prompt, chs_short_system_prompt\n",
    "        else:\n",
    "            return cht_full_system_prompt, cht_short_system_prompt\n",
    "\n",
    "    def get_sorted_prompts_for_language(language):\n",
    "        if language == \"简体中文\":\n",
    "            return sorted_chs_prompts\n",
    "        else:\n",
    "            return sorted_cht_prompts\n",
    "\n",
    "    def pad_with_cached_prompts(current_tokens, padding_length, language):\n",
    "        \"\"\"使用缓存的prompt进行填充，避免过度使用pad_token_id\"\"\"\n",
    "        sorted_prompts = get_sorted_prompts_for_language(language)\n",
    "\n",
    "        # 由于sorted_prompts已按长度排序，使用二分查找找到合适的范围\n",
    "        # 找到第一个长度大于padding_length的索引\n",
    "        suitable_end_idx = 0\n",
    "        for i, prompt in enumerate(sorted_prompts):\n",
    "            if len(prompt) > padding_length:\n",
    "                break\n",
    "            suitable_end_idx = i + 1\n",
    "\n",
    "        # 获取所有合适长度的prompts\n",
    "        suitable_prompts = sorted_prompts[:suitable_end_idx].copy()\n",
    "\n",
    "        while padding_length > 0 and suitable_prompts:\n",
    "            # 随机选择一个合适的prompt\n",
    "            selected_prompt = random.choice(suitable_prompts)\n",
    "\n",
    "            current_tokens.extend(selected_prompt)\n",
    "            padding_length -= len(selected_prompt)\n",
    "\n",
    "            # 移除已使用的prompt，避免重复使用\n",
    "            suitable_prompts.remove(selected_prompt)\n",
    "\n",
    "            # 更新suitable_prompts，移除现在长度超过剩余padding_length的prompts\n",
    "            suitable_prompts = [p for p in suitable_prompts if len(p) <= padding_length]\n",
    "\n",
    "        # 如果还有剩余长度，用pad_token_id填充\n",
    "        if padding_length > 0:\n",
    "            current_tokens.extend([tokenizer.pad_token_id] * padding_length)\n",
    "\n",
    "        return current_tokens\n",
    "\n",
    "    # 周期性系统提示词策略配置\n",
    "    full_system_interval = 4  # 每4个样本使用一次完整系统提示词\n",
    "    chs_index = 0\n",
    "    cht_index = 0\n",
    "    samples_in_current_sequence = 0  # 当前序列中的样本数量\n",
    "\n",
    "    # 按原始顺序遍历examples，使用已处理的prompts\n",
    "    for i, example in enumerate(examples):\n",
    "        target_language = extract_target_language(example[\"userprompt\"])\n",
    "\n",
    "        # 获取对应语言的完整和简短系统提示词\n",
    "        full_system_prompt, short_system_prompt = get_system_prompts_for_language(target_language)\n",
    "\n",
    "        # 使用已处理的prompt数据\n",
    "        if target_language == \"简体中文\":\n",
    "            processed_prompt_tokens = chs_prompts[chs_index]\n",
    "            chs_index += 1\n",
    "        else:\n",
    "            processed_prompt_tokens = cht_prompts[cht_index]\n",
    "            cht_index += 1\n",
    "\n",
    "        # 检查是否需要开始新序列（语言变化或长度超限）\n",
    "        is_first_in_sequence = False\n",
    "        if current_language is None or current_language != target_language:\n",
    "            is_first_in_sequence = True\n",
    "            current_language = target_language\n",
    "            samples_in_current_sequence = 0\n",
    "\n",
    "        # 决定使用哪种系统提示词\n",
    "        if is_first_in_sequence or samples_in_current_sequence % full_system_interval == 0:\n",
    "            # 使用完整系统提示词（序列开始或周期性强化）\n",
    "            system_prompt_to_use = full_system_prompt\n",
    "        else:\n",
    "            # 使用简短系统提示词\n",
    "            system_prompt_to_use = short_system_prompt\n",
    "\n",
    "        # 构建tokens\n",
    "        if is_first_in_sequence:\n",
    "            # 第一个样本包含系统提示词\n",
    "            tokens = system_prompt_to_use + processed_prompt_tokens\n",
    "            samples_in_current_sequence = 1\n",
    "        else:\n",
    "            # 检查是否需要添加系统提示词（周期性或长度限制）\n",
    "            if samples_in_current_sequence % full_system_interval == 0:\n",
    "                # 周期性添加完整系统提示词\n",
    "                tokens = system_prompt_to_use + processed_prompt_tokens\n",
    "            else:\n",
    "                # 只添加简短系统提示词\n",
    "                tokens = system_prompt_to_use + processed_prompt_tokens\n",
    "\n",
    "            # 检查长度是否超限\n",
    "            if (len(current_tokens) + len(tokens) > max_length):\n",
    "                is_first_in_sequence = True\n",
    "                # 超长时重新开始，使用完整系统提示词\n",
    "                tokens = full_system_prompt + processed_prompt_tokens\n",
    "                samples_in_current_sequence = 1\n",
    "            else:\n",
    "                samples_in_current_sequence += 1\n",
    "\n",
    "        # 检查是否需要开始新的序列\n",
    "        if is_first_in_sequence:\n",
    "            # 保存当前序列（如果有内容）\n",
    "            if len(current_tokens) > 0:\n",
    "                # 第四步：使用改进的填充策略\n",
    "                padding_length = max_length - len(current_tokens)\n",
    "                if padding_length > 0:\n",
    "                    current_tokens = pad_with_cached_prompts(current_tokens, padding_length, current_language)\n",
    "                text = tokenizer.decode(torch.tensor(current_tokens))\n",
    "                packed_inputs.append({\"text\": text})\n",
    "\n",
    "            # 开始新的序列\n",
    "            current_tokens = tokens.copy()\n",
    "        else:\n",
    "            # 添加到当前序列\n",
    "            current_tokens.extend(tokens)\n",
    "\n",
    "    # 处理最后一个序列\n",
    "    if len(current_tokens) > 0:\n",
    "        padding_length = max_length - len(current_tokens)\n",
    "        if padding_length > 0:\n",
    "            current_tokens = pad_with_cached_prompts(current_tokens, padding_length, current_language)\n",
    "        text = tokenizer.decode(torch.tensor(current_tokens))\n",
    "        packed_inputs.append({\"text\": text})\n",
    "\n",
    "    return packed_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T02:08:41.492731Z",
     "iopub.status.busy": "2025-04-17T02:08:41.492482Z",
     "iopub.status.idle": "2025-04-17T02:08:41.506928Z",
     "shell.execute_reply": "2025-04-17T02:08:41.505830Z",
     "shell.execute_reply.started": "2025-04-17T02:08:41.492707Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def storage_ptr(tensor):\n",
    "    return tensor.untyped_storage().data_ptr() if tensor.is_cuda or tensor.device.type == 'cpu' else 0\n",
    "\n",
    "def storage_size(tensor):\n",
    "    return tensor.untyped_storage().nbytes() if tensor.is_cuda or tensor.device.type == 'cpu' else 0\n",
    "\n",
    "def check_weights_on_cpu_with_storage(state_dict):\n",
    "    for name, tensor in state_dict.items():\n",
    "        # print(f\"Parameter '{name}' 的设备: {tensor.device}\")\n",
    "        # 检查设备是否为 CPU\n",
    "        if tensor.device.type != 'cpu':\n",
    "            return False, f\"Parameter '{name}' is on device {tensor.device}, not CPU.\"\n",
    "        # 检查存储指针和大小是否有效\n",
    "        if storage_ptr(tensor) == 0:\n",
    "            return False, f\"Parameter '{name}' has invalid storage pointer (0).\"\n",
    "        if storage_size(tensor) == 0:\n",
    "            return False, f\"Parameter '{name}' has invalid storage size (0 bytes).\"\n",
    "    return True, \"All weights are on CPU with valid storage.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T02:08:41.508153Z",
     "iopub.status.busy": "2025-04-17T02:08:41.507913Z",
     "iopub.status.idle": "2025-04-17T02:08:41.522633Z",
     "shell.execute_reply": "2025-04-17T02:08:41.521890Z",
     "shell.execute_reply.started": "2025-04-17T02:08:41.508129Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import time\n",
    "\n",
    "# 修改自定义检查点保存回调\n",
    "# 修改自定义检查点保存回调\n",
    "class CustomCheckpointCallback(TrainerCallback):\n",
    "    \"\"\"自定义检查点保存回调，根据评估损失决定是否保存模型\"\"\"\n",
    "    \n",
    "    def __init__(self, patience=3, tokenizer=None):\n",
    "        self.best_score = float('inf')  # 初始化为无穷大,因为要最小化损失\n",
    "        self.patience = patience\n",
    "        self.no_improvement_count = 0\n",
    "        self.tokenizer = tokenizer\n",
    "        self.best_step = None  # 记录最佳步骤\n",
    "        self.best_state_dict = None  # 用于缓存最佳模型的权重\n",
    "    \n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        if not metrics:\n",
    "            return\n",
    "        \n",
    "        # 获取当前评估损失\n",
    "        current_loss = metrics.get(\"eval_loss\", float('inf'))\n",
    "        # 获取当前步骤\n",
    "        current_step = state.global_step\n",
    "        \n",
    "        print(f\"\\n当前步骤: {current_step}, 当前评估损失: {current_loss}, 最佳损失: {self.best_score}\")\n",
    "        \n",
    "        # 如果当前损失小于最佳损失,可能需要保存模型\n",
    "        if current_loss < self.best_score:\n",
    "            print(f\"发现更好的模型 (Loss: {current_loss} < {self.best_score})\")\n",
    "\n",
    "            # 更新最佳分数和重置耐心计数器\n",
    "            self.best_score = current_loss\n",
    "            self.no_improvement_count = 0\n",
    "            self.best_step = current_step\n",
    "            \n",
    "            # 获取模型\n",
    "            model = kwargs.get(\"model\")\n",
    "            # 清理之前的缓存（如果有）\n",
    "            if self.best_state_dict is not None:\n",
    "                del self.best_state_dict\n",
    "                # 强制进行垃圾回收\n",
    "                import gc\n",
    "                gc.collect()\n",
    "            \n",
    "            # 将当前模型权重缓存到内存中\n",
    "            self._cache_model_weights(model)\n",
    "\n",
    "        else:\n",
    "            # 如果没有改进，增加计数器\n",
    "            self.no_improvement_count += 1\n",
    "            print(f\"没有改进，当前耐心计数: {self.no_improvement_count}/{self.patience}\")\n",
    "            \n",
    "            # 如果超过耐心值，可以提前停止训练\n",
    "            if self.no_improvement_count >= self.patience:\n",
    "                print(f\"已经 {self.patience} 次评估没有改进，提前停止训练\")\n",
    "                control.should_training_stop = True\n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        \"\"\"训练结束时将最佳模型权重保存到磁盘\"\"\"\n",
    "        if self.best_state_dict is not None:\n",
    "            print(f\"\\n训练结束，保存最佳模型 (步骤: {self.best_step}, 损失: {self.best_score})\")\n",
    "                \n",
    "        try:\n",
    "            output_dir = \"./model\"\n",
    "            \n",
    "            # 如果目标目录已存在则先删除\n",
    "            if os.path.exists(output_dir):\n",
    "                print(f\"删除现有目标目录: {output_dir}\")\n",
    "                shutil.rmtree(output_dir)\n",
    "            \n",
    "            # 创建输出目录\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "            # 保存模型权重\n",
    "            print(f\"开始保存模型权重到: {output_dir}\")\n",
    "            \n",
    "            # 使用PeftModel的save_pretrained方法保存权重\n",
    "            self._save_cached_weights(kwargs.get(\"model\"), output_dir)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"保存最佳模型时出错: {str(e)}\")\n",
    "            \n",
    "        finally:\n",
    "            # 清理内存\n",
    "            del self.best_state_dict\n",
    "            import gc\n",
    "            gc.collect()\n",
    "    \n",
    "    def _cache_model_weights(self, model):\n",
    "        \"\"\"将模型权重缓存到内存中而不是保存到磁盘\"\"\"\n",
    "        try:\n",
    "            # 确保我们有模型实例\n",
    "            if model is None:\n",
    "                print(\"错误：无法获取模型实例\")\n",
    "                return\n",
    "                \n",
    "            print(\"正在将模型权重缓存到CPU内存...\")\n",
    "            \n",
    "            # 记录开始时间\n",
    "            extract_start_time = time.time()\n",
    "            # 提取原始模型（非并行包装）\n",
    "            unwrap = extract_model_from_parallel(model, recursive=True)\n",
    "\n",
    "            state_dict = unwrap.state_dict()\n",
    "            to_cpu = {}\n",
    "            size = 0\n",
    "            for name, param in unwrap.named_parameters():\n",
    "                # 检查是否存在\n",
    "                if name in state_dict:\n",
    "                    del state_dict[name]\n",
    "                else:\n",
    "                    print(f\"参数 {name} 不存在于state_dict中\")\n",
    "\n",
    "                if  \"lora_\" in name  or  param.requires_grad:\n",
    "                    to_cpu[name] = param.data\n",
    "                    size += param.data.numel() * param.data.element_size()\n",
    "            # 打印state_dict剩余的参数\n",
    "            if len(state_dict) > 0:\n",
    "                print(f\"state_dict剩余的参数: {state_dict.keys()}\")\n",
    "\n",
    "            self.best_state_dict = xm._maybe_convert_to_cpu(to_cpu)\n",
    "            convert_end_time = time.time()\n",
    "            print(f\"TPU权重转移到CPU耗时: {convert_end_time - extract_start_time:.2f}秒，大小: {size / (1024 * 1024):.2f}MB\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"缓存模型权重时出错: {str(e)}\")\n",
    "            self.best_state_dict = None\n",
    "    \n",
    "    def _save_cached_weights(self, model, output_dir):\n",
    "        try:\n",
    "            # 确保我们有模型实例\n",
    "            if model is None:\n",
    "                print(\"错误：无法获取模型实例\")\n",
    "                return\n",
    "\n",
    "            # 记录模型提取时间\n",
    "            start_time = time.time()\n",
    "            unwrap = extract_model_from_parallel(model,recursive=True)\n",
    "            \"\"\"将缓存的模型权重保存到磁盘\"\"\"\n",
    "            if self.best_state_dict is None:\n",
    "                print(\"从TPU获取模型状态字典...\")\n",
    "                state_dict = xm._maybe_convert_to_cpu(unwrap.state_dict())\n",
    "            else:\n",
    "                print(\"使用缓存的模型状态字典...\")\n",
    "                state_dict = self.best_state_dict\n",
    "            dict_end_time = time.time()\n",
    "            print(f\"获取/准备模型状态字典耗时: {dict_end_time - start_time:.2f}秒\")\n",
    "        \n",
    "            # 记录保存时间\n",
    "            save_start_time = time.time()\n",
    "            unwrap.save_pretrained(\n",
    "                    output_dir,\n",
    "                    save_function=xm.save,\n",
    "                    state_dict=state_dict,\n",
    "            )\n",
    "            save_end_time = time.time()\n",
    "            print(f\"save_pretrained调用耗时: {save_end_time - save_start_time:.2f}秒\")\n",
    "            \n",
    "            # 总耗时\n",
    "            total_time = time.time() - start_time\n",
    "            print(f\"模型检查点已成功保存到 {output_dir}，总耗时: {total_time:.2f}秒\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"保存LoRA权重时出错: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T02:08:41.524007Z",
     "iopub.status.busy": "2025-04-17T02:08:41.523771Z",
     "iopub.status.idle": "2025-04-17T02:08:41.538651Z",
     "shell.execute_reply": "2025-04-17T02:08:41.537600Z",
     "shell.execute_reply.started": "2025-04-17T02:08:41.523981Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from typing import Any, Union\n",
    "\n",
    "class DataCollatorForCompletionOnlyLM(DataCollatorForLanguageModeling):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args,\n",
    "        mlm: bool = False,\n",
    "        ignore_index: int = -100,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, mlm=mlm, **kwargs)\n",
    "        self.ignore_index = ignore_index\n",
    "        \n",
    "        # 定义标记用于定位assistant回复\n",
    "        self.assistant_start = \"<|im_start|>assistant\\n\"\n",
    "        self.im_end = \"<|im_end|>\"\n",
    "        \n",
    "        # 获取标记的token ids\n",
    "        self.assistant_start_tokens = self.tokenizer.encode(self.assistant_start, add_special_tokens=False)\n",
    "        self.im_end_tokens = self.tokenizer.encode(self.im_end, add_special_tokens=False)\n",
    "        print(f\"assistant_start_tokens: {self.assistant_start_tokens}\")\n",
    "        print(f\"im_end_tokens: {self.im_end_tokens}\")\n",
    "    \n",
    "\n",
    "    def torch_call(self, examples: list[Union[list[int], Any, dict[str, Any]]]) -> dict[str, Any]:\n",
    "        batch = super().torch_call(examples)\n",
    "\n",
    "        for i in range(len(examples)):\n",
    "            # 寻找assistant回复的开始位置\n",
    "            start_token = self.assistant_start_tokens[0]\n",
    "            start_indices = np.where(batch[\"labels\"][i] == start_token)[0]\n",
    "            end_token = self.im_end_tokens[0]\n",
    "            prev_end_idx = 0\n",
    "            \n",
    "            for start_idx in start_indices:\n",
    "                if start_idx < prev_end_idx:\n",
    "                    continue\n",
    "\n",
    "                # 验证完整的assistant_start_tokens序列\n",
    "                if (self.assistant_start_tokens == \n",
    "                    batch[\"labels\"][i][start_idx:start_idx + len(self.assistant_start_tokens)].tolist()):\n",
    "                    start_pos = start_idx + len(self.assistant_start_tokens)\n",
    "                    \n",
    "                    # 从start_pos开始寻找im_end标记\n",
    "                    end_indices = np.where(batch[\"labels\"][i][start_pos:] == end_token)[0]\n",
    "                    \n",
    "                    for relative_end_idx in end_indices:\n",
    "                        end_idx = start_pos + relative_end_idx\n",
    "                        # 验证完整的im_end_tokens序列\n",
    "                        if (self.im_end_tokens == \n",
    "                            batch[\"labels\"][i][end_idx:end_idx + len(self.im_end_tokens)].tolist()):\n",
    "                            # 只保留assistant回复部分的标签，其他部分设为ignore_index\n",
    "                            batch[\"labels\"][i, prev_end_idx:start_pos] = self.ignore_index\n",
    "                            prev_end_idx = end_idx+len(self.im_end_tokens)\n",
    "                            break\n",
    "            if prev_end_idx < len(batch[\"labels\"][i]):\n",
    "                batch[\"labels\"][i, prev_end_idx:] = self.ignore_index\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T02:08:41.539919Z",
     "iopub.status.busy": "2025-04-17T02:08:41.539660Z",
     "iopub.status.idle": "2025-04-17T02:08:42.958801Z",
     "shell.execute_reply": "2025-04-17T02:08:42.957672Z",
     "shell.execute_reply.started": "2025-04-17T02:08:41.539897Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "bleu = evaluate.load(\"sacrebleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "    \n",
    "# 修改评估指标计算函数\n",
    "def compute_metrics(eval_preds):\n",
    "\n",
    "    # SFTTrainer的eval_preds是一个EvalPrediction对象\n",
    "    # 其中包含predictions和label_ids\n",
    "    predictions =eval_preds.predictions\n",
    "    labels = eval_preds.label_ids\n",
    "    \n",
    "    decoded_preds = []\n",
    "    decoded_labels = []\n",
    "    \n",
    "    for item1, item2 in zip(predictions, labels):\n",
    "        item1 = np.argmax(item1, axis=-1)\n",
    "        \n",
    "        for seq1, seq2 in zip(item1, item2):\n",
    "            # 只取labels中非-100的位置\n",
    "            valid_positions = seq2 != -100\n",
    "            valid_pred = seq1[valid_positions]\n",
    "            valid_label = seq2[valid_positions]\n",
    "            \n",
    "            # 解码有效位置的token\n",
    "            pred_text = tokenizer.decode(valid_pred, skip_special_tokens=True)\n",
    "            label_text = tokenizer.decode(valid_label, skip_special_tokens=True)\n",
    "            \n",
    "            if pred_text.strip() and label_text.strip():  # 只添加非空文本\n",
    "                decoded_preds.append(pred_text)\n",
    "                decoded_labels.append(label_text)\n",
    "    \n",
    "    # 对中文文本进行分词\n",
    "    decoded_preds = [' '.join(jieba.cut(pred)) for pred in decoded_preds]\n",
    "    decoded_labels = [' '.join(jieba.cut(label)) for label in decoded_labels]\n",
    "    \n",
    "    # 计算BLEU分数\n",
    "    bleu_score = bleu.compute(predictions=decoded_preds, references=[[ref] for ref in decoded_labels])\n",
    "    print(f\"bleu_score: {bleu_score}\")\n",
    "    # 计算ROUGE分数\n",
    "    rouge_scores = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    print(f\"rouge_scores: {rouge_scores}\")\n",
    "    \n",
    "    return {\n",
    "        'bleu': bleu_score['score'],\n",
    "        'rouge1': round(float(rouge_scores['rouge1']), 4),\n",
    "        'rouge2': round(float(rouge_scores['rouge2']), 4),\n",
    "        'rougeL': round(float(rouge_scores['rougeL']), 4),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T02:08:42.961225Z",
     "iopub.status.busy": "2025-04-17T02:08:42.960618Z",
     "iopub.status.idle": "2025-04-17T02:08:42.970792Z",
     "shell.execute_reply": "2025-04-17T02:08:42.969842Z",
     "shell.execute_reply.started": "2025-04-17T02:08:42.961199Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    fsdp_v2.use_fsdp_v2()\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)\n",
    "    \n",
    "    fsdp_training_args = {\n",
    "        \"fsdp\": \"full_shard auto_wrap\",  # 启用全分片策略\n",
    "        \"fsdp_config\": {\n",
    "            \"transformer_layer_cls_to_wrap\": [\"Qwen2DecoderLayer\"],\n",
    "            \"xla\": True,\n",
    "            \"xla_fsdp_v2\": True,\n",
    "            \"xla_fsdp_grad_ckpt\": True \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "            r=128,\n",
    "            lora_alpha=256,\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            target_modules=[\n",
    "                \"q_proj\", \n",
    "                \"v_proj\",\n",
    "                \"o_proj\", \n",
    "                \"gate_proj\",\n",
    "                \"up_proj\",\n",
    "                \"down_proj\"\n",
    "            ],\n",
    "            modules_to_save=[\"lm_head\"],\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    \n",
    "    train_data, val_data = load_corpus('./translation_dataset.json')\n",
    "    train_dataset = Dataset.from_list(pack_sequences(train_data, tokenizer, max_length=1024))\n",
    "    val_dataset = Dataset.from_list(pack_sequences(val_data, tokenizer, max_length=1024))\n",
    "    \n",
    "    training_args = SFTConfig(\n",
    "        max_seq_length=1024,\n",
    "        # packing=True,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=10,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=10,\n",
    "        eval_do_concat_batches=False,\n",
    "        save_strategy=\"no\",\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        learning_rate=2e-5,\n",
    "        warmup_ratio=0.1,\n",
    "        weight_decay=0.01,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        # metric_for_best_model=\"bleu\",   # 使用BLEU分数作为最佳模型选择标准\n",
    "        # greater_is_better=True,         # BLEU分数越高越好\n",
    "        output_dir=\"./tmp/output\",\n",
    "        optim=\"adamw_torch_xla\",\n",
    "        logging_steps=1,\n",
    "        dataloader_drop_last=True,  # Required by FSDP v2\n",
    "        **fsdp_training_args,\n",
    "    )\n",
    "\n",
    "    checkpoint_callback = CustomCheckpointCallback(\n",
    "        patience=3,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    completion_collator = DataCollatorForCompletionOnlyLM(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,\n",
    "    )\n",
    "    \n",
    "    # 在trainer初始化时添加回调\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        args=training_args,\n",
    "        processing_class=tokenizer,\n",
    "        peft_config=lora_config,\n",
    "        data_collator=completion_collator,\n",
    "        # compute_metrics=compute_metrics,  # 添加评估指标计算函数\n",
    "        callbacks=[checkpoint_callback]  # 添加自定义检查点回调\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T02:08:42.972214Z",
     "iopub.status.busy": "2025-04-17T02:08:42.971803Z",
     "iopub.status.idle": "2025-04-17T02:30:41.751836Z",
     "shell.execute_reply": "2025-04-17T02:30:41.750575Z",
     "shell.execute_reply.started": "2025-04-17T02:08:42.972191Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch_xla.runtime as xr\n",
    "xr.initialize_cache('xla_cache', readonly=False)\n",
    "\n",
    "trainer = train()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 12983869,
     "datasetId": 6269030,
     "sourceId": 12416413,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 12066747,
     "modelInstanceId": 301517,
     "sourceId": 363139,
     "sourceType": "modelInstanceVersion"
    },
    {
     "databundleVersionId": 10159036,
     "modelInstanceId": 141469,
     "sourceId": 166258,
     "sourceType": "modelInstanceVersion"
    },
    {
     "databundleVersionId": 10158882,
     "modelInstanceId": 141458,
     "sourceId": 166245,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
