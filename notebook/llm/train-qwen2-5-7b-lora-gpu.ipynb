{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-26T02:38:52.131475Z",
     "iopub.status.busy": "2025-02-26T02:38:52.131175Z",
     "iopub.status.idle": "2025-02-26T02:39:14.449397Z",
     "shell.execute_reply": "2025-02-26T02:39:14.448631Z",
     "shell.execute_reply.started": "2025-02-26T02:38:52.131450Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install peft accelerate bitsandbytes evaluate sentencepiece scipy transformers[deepspeed] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T02:39:14.451089Z",
     "iopub.status.busy": "2025-02-26T02:39:14.450785Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "import json\n",
    "import random\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "# 初始化 Secrets 客户端\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "# 获取密钥（键名需与 Secrets 中设置的一致）\n",
    "wandb_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "\n",
    "# 设置为环境变量\n",
    "os.environ[\"WANDB_API_KEY\"] = wandb_key\n",
    "os.environ[\"HF_TOKEN\"] = hf_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wandb.login(key = wandb_key)\n",
    "run = wandb.init(\n",
    "    project='Fine tuning Qwen 7B', \n",
    "    job_type=\"training\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_corpus(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    def wrap_prompt(src, tgt, lang):\n",
    "        # 构建统一的系统提示\n",
    "        system_prompt = f\"\"\"<|im_start|>system\n",
    "你是专注于xxx领域的资深翻译专家，专门负责将英文文档精准翻译成 {lang}。\n",
    "\n",
    "## 翻译原则：\n",
    "\n",
    "### 1. 术语准确性\n",
    "- 严格使用xxx行业标准术语\n",
    "- 保持技术参数的精确性和专业性\n",
    "- 遵循xxx领域的权威表达方式\n",
    "\n",
    "### 2. 句式结构与技术逻辑忠实性\n",
    "- 英文常见被动语态，在{lang}中需转换为符合习惯的**主动语态或自然表述**，避免生硬直译\n",
    "- 对复杂英文长句，按{lang}习惯进行合理拆分或重组，确保**逻辑清晰、易于理解**，同时**不丢失任何细节**\n",
    "- 逻确保技术逻辑连接词翻译准确，因果关系、条件关系明确\n",
    "\n",
    "**请逐句审阅，严格遵循以上所有规范。**<|im_end|>\"\"\"\n",
    "\n",
    "        user_prompt = f\"<|im_start|>user\\n### 请将以下文本准确翻译成{lang}：\\n{src}/no_think<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "\n",
    "        # 构建助手回复\n",
    "        assistant_response = f\"{tgt}<|im_end|>\"\n",
    "\n",
    "        return {\n",
    "                \"systemprompt\": system_prompt,\n",
    "                \"userprompt\": user_prompt,\n",
    "                \"prompt\": f\"{system_prompt}\\n{user_prompt}\",\n",
    "                \"completion\": assistant_response,\n",
    "            }\n",
    "\n",
    "    def process_pairs(pairs, shuffle=True):\n",
    "        # 构建ChatML格式的数据，按语言类型分组\n",
    "        zhcn_samples = []\n",
    "        zhtw_samples = []\n",
    "\n",
    "        for src, tgt, *tgt2 in pairs:\n",
    "            zhcn_samples.append(wrap_prompt(src, tgt, \"简体中文\"))\n",
    "            if len(tgt2) > 0:\n",
    "                zhtw_samples.append(wrap_prompt(src, tgt2[0], \"繁体中文\"))\n",
    "\n",
    "        # 按语言类型分组，避免在同一序列中混合不同语言\n",
    "        processed = []\n",
    "        if shuffle:\n",
    "            random.shuffle(zhcn_samples)\n",
    "            random.shuffle(zhtw_samples)\n",
    "        # 先添加简体中文样本\n",
    "        processed.extend(zhcn_samples)\n",
    "        # 再添加繁体中文样本\n",
    "        processed.extend(zhtw_samples)\n",
    "\n",
    "        return processed\n",
    "    \n",
    "    train_data = process_pairs(data['train'])\n",
    "    val_data = process_pairs(data['validation'],False)\n",
    "    \n",
    "    return train_data, val_data\n",
    "\n",
    "def create_datasets(train_data, val_data, tokenizer, accelerator):\n",
    "    \"\"\"创建数据集\"\"\"\n",
    "    def tokenize_function(examples):\n",
    "        # 分别处理输入和输出\n",
    "        inputs = examples[\"prompt\"]\n",
    "        completion = examples[\"completion\"]\n",
    "        \n",
    "        # 构建完整的prompt\n",
    "        prompts = []\n",
    "        labels = []\n",
    "        \n",
    "        for inp, out in zip(inputs, completion):\n",
    "            # 对输入进行tokenize\n",
    "            input_ids = tokenizer(\n",
    "                inp,\n",
    "                truncation=True,\n",
    "                max_length=256,  # 减小长度避免过长序列\n",
    "                add_special_tokens=False,  # 不添加特殊token\n",
    "            )[\"input_ids\"]\n",
    "            \n",
    "            # 对输出进行tokenize\n",
    "            output_ids = tokenizer(\n",
    "                out,\n",
    "                truncation=True,\n",
    "                max_length=256,\n",
    "                add_special_tokens=False,\n",
    "            )[\"input_ids\"]\n",
    "            \n",
    "            # 合并prompt\n",
    "            prompts.append(inp + out)\n",
    "            \n",
    "            # 构建标签，输入部分用-100，输出部分用实际token id\n",
    "            label = [-100] * len(input_ids) + output_ids\n",
    "            labels.append(label)\n",
    "        \n",
    "        # 对所有序列进行padding\n",
    "        model_inputs = tokenizer(\n",
    "            prompts,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        # 对标签进行padding\n",
    "        labels_padded = torch.full((len(labels), 512), -100)  # 默认填充-100\n",
    "        for i, label in enumerate(labels):\n",
    "            length = min(len(label), 512)  # 避免超出最大长度\n",
    "            labels_padded[i, :length] = torch.tensor(label[:length])\n",
    "        \n",
    "        model_inputs[\"labels\"] = labels_padded\n",
    "        \n",
    "        return model_inputs\n",
    "\n",
    "    # 转换为HuggingFace数据集格式\n",
    "    train_dataset = Dataset.from_list(train_data)\n",
    "    val_dataset = Dataset.from_list(val_data)\n",
    "    \n",
    "    # 使用 accelerator 控制数据集处理\n",
    "    with accelerator.main_process_first():\n",
    "        train_dataset = train_dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            num_proc=1,\n",
    "            remove_columns=train_dataset.column_names,\n",
    "            load_from_cache_file=False,\n",
    "            desc=\"Processing training dataset\",\n",
    "        )\n",
    "        \n",
    "        val_dataset = val_dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            num_proc=1,\n",
    "            remove_columns=val_dataset.column_names,\n",
    "            load_from_cache_file=False,\n",
    "            desc=\"Processing validation dataset\",\n",
    "        )\n",
    "    \n",
    "    # 等待所有进程完成数据处理\n",
    "    accelerator.wait_for_everyone()\n",
    "    \n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def peft_config():\n",
    "    \"\"\"创建LoRA配置\"\"\"\n",
    "    return LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=64,  # 降低LoRA秩以减少参数量\n",
    "        lora_alpha=128,\n",
    "        lora_dropout=0.05,\n",
    "        # 针对Qwen的结构调整target_modules\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            # \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            # \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            # \"up_proj\",\n",
    "            \"down_proj\"\n",
    "        ],\n",
    "        bias=\"none\",\n",
    "        # fan_in_fan_out=True  # 适配并行模式\n",
    "    )\n",
    "\n",
    "def deepspeed_config():\n",
    "    \"\"\"创建 DeepSpeed 配置\"\"\"\n",
    "    return {\n",
    "        \"is_deepspeed_zero3_enabled\": True,\n",
    "        \"fp16\": {\n",
    "            \"enabled\": True,\n",
    "            \"loss_scale\": 0,\n",
    "            \"loss_scale_window\": 1000,\n",
    "            \"initial_scale_power\": 16,\n",
    "            \"hysteresis\": 2,\n",
    "            \"min_loss_scale\": 1\n",
    "        },\n",
    "        \"zero_optimization\": {\n",
    "            \"stage\": 3,\n",
    "            \"offload_optimizer\": {\n",
    "                \"device\": \"cpu\",\n",
    "                \"pin_memory\": True\n",
    "            },\n",
    "            \"offload_param\": {\n",
    "                \"device\": \"cpu\",\n",
    "                \"pin_memory\": True\n",
    "            },\n",
    "            \"overlap_comm\": True,\n",
    "            \"contiguous_gradients\": True,\n",
    "            \"reduce_bucket_size\": \"auto\",\n",
    "            \"stage3_prefetch_bucket_size\": \"auto\",\n",
    "            \"stage3_param_persistence_threshold\": \"auto\",\n",
    "            \"stage3_max_live_parameters\": 1e9,\n",
    "            \"stage3_max_reuse_distance\": 1e9,\n",
    "            \"stage3_gather_16bit_weights_on_model_save\": True,\n",
    "        },\n",
    "        # \"gradient_accumulation_steps\": \"auto\",  # 让 DeepSpeed 自动适应\n",
    "        \"gradient_clipping\": \"auto\",\n",
    "        # \"train_micro_batch_size_per_gpu\": \"auto\",  # 让 DeepSpeed 自动适应\n",
    "        # \"train_batch_size\": \"auto\",  # 让 DeepSpeed 自动适应\n",
    "        \"train_micro_batch_size_per_gpu\": 1,\n",
    "        \"train_batch_size\": 2,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    try:\n",
    "        accelerator = Accelerator()\n",
    "        accelerator.print(\"\\n=== 初始化训练 ===\")\n",
    "        accelerator.print(f\"进程数: {accelerator.num_processes}\")\n",
    "        accelerator.print(f\"分布式环境: {accelerator.distributed_type}\")\n",
    "\n",
    "        model_name= \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "        \n",
    "        # 配置4bit量化\n",
    "        accelerator.print(\"\\n=== 配置量化参数 ===\")\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_quant_storage=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            llm_int8_threshold=10.0\n",
    "        )\n",
    "\n",
    "        output_dir = 'cpt'\n",
    "        ds_config = deepspeed_config()\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            num_train_epochs=3,\n",
    "            per_device_train_batch_size=1,\n",
    "            per_device_eval_batch_size=1,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=100,\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=100,\n",
    "            save_total_limit=3,\n",
    "            # learning_rate=1e-5,  # 降低学习率\n",
    "            # weight_decay=0.001,  # 减小权重衰减\n",
    "            # warmup_ratio=0.1,\n",
    "            # max_grad_norm=0.3,  # 添加梯度裁剪\n",
    "            fp16=True,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            push_to_hub=False,\n",
    "            # gradient_checkpointing=True,\n",
    "            # report_to=[\"none\"],\n",
    "            optim=\"adamw_torch\",\n",
    "            resume_from_checkpoint=True,\n",
    "            deepspeed=ds_config,\n",
    "            local_rank=-1,\n",
    "        )\n",
    "\n",
    "        # 加载模型\n",
    "        accelerator.print(\"\\n=== 加载模型 ===\")\n",
    "        try:\n",
    "            import deepspeed\n",
    "\n",
    "            # with deepspeed.zero.Init(config_dict_or_path=ds_config):\n",
    "            with deepspeed.zero.Init():\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    # quantization_config=quantization_config,\n",
    "                    # device_map=\"auto\",\n",
    "                    # low_cpu_mem_usage=True,\n",
    "                    trust_remote_code=True,\n",
    "                    torch_dtype=torch.float16\n",
    "                )\n",
    "                model = get_peft_model(model, peft_config())\n",
    "                accelerator.print(\"模型加载成功\")\n",
    "        except Exception as e:\n",
    "            accelerator.print(f\"模型加载失败: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        # 准备 LoRA 训练\n",
    "        accelerator.print(\"\\n=== 配置 LoRA ===\")\n",
    "        try:\n",
    "            # model = prepare_model_for_kbit_training(model,use_gradient_checkpointing=False)\n",
    "            model = get_peft_model(model, peft_config())\n",
    "            accelerator.print(\"LoRA 配置成功\")\n",
    "        except Exception as e:\n",
    "            accelerator.print(f\"LoRA 配置失败: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        # 加载 tokenizer\n",
    "        accelerator.print(\"\\n=== 加载 Tokenizer ===\")\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_name,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            accelerator.print(\"Tokenizer 加载成功\")\n",
    "        except Exception as e:\n",
    "            accelerator.print(f\"Tokenizer 加载失败: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        # 加载数据集\n",
    "        accelerator.print(\"\\n=== 加载数据集 ===\")\n",
    "        try:\n",
    "            train_data, val_data = load_corpus('./translation_dataset.json')\n",
    "            train_dataset, val_dataset = create_datasets(train_data, val_data, tokenizer, accelerator)\n",
    "        except Exception as e:\n",
    "            accelerator.print(f\"数据集加载失败: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "        # 配置训练器\n",
    "        accelerator.print(\"\\n=== 配置训练器 ===\")\n",
    "        try:\n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=val_dataset,\n",
    "                tokenizer=tokenizer,\n",
    "            )\n",
    "            accelerator.print(\"训练器配置成功\")\n",
    "        except Exception as e:\n",
    "            accelerator.print(f\"训练器配置失败: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        # 开始训练\n",
    "        accelerator.print(\"\\n=== 开始训练 ===\")\n",
    "        try:\n",
    "            trainer.train()\n",
    "            accelerator.print(\"训练完成\")\n",
    "        except Exception as e:\n",
    "            accelerator.print(f\"训练过程出错: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        # 保存模型\n",
    "        accelerator.print(\"\\n=== 保存模型 ===\")\n",
    "        try:\n",
    "            output_dir=\"model\"\n",
    "            trainer.save_model(output_dir)\n",
    "            accelerator.print(\"模型保存成功\")\n",
    "        except Exception as e:\n",
    "            accelerator.print(f\"模型保存失败: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    except Exception as e:\n",
    "        accelerator.print(f\"\\n=== 训练异常 ===\\n{str(e)}\")\n",
    "        # 打印完整的错误堆栈\n",
    "        import traceback\n",
    "        accelerator.print(traceback.format_exc())\n",
    "        raise\n",
    "    finally:\n",
    "        # 清理资源\n",
    "        accelerator.print(\"\\n=== 清理资源 ===\")\n",
    "        if 'model' in locals():\n",
    "            del model\n",
    "        if 'trainer' in locals():\n",
    "            del trainer\n",
    "        torch.cuda.empty_cache()\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        accelerator.print(\"资源清理完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "print(torch.cuda.is_initialized())\n",
    "\n",
    "notebook_launcher(\n",
    "    train,  # 训练函数\n",
    "    args=(),  # 空元组，因为train函数不需要额外参数\n",
    "    num_processes=2,  # 使用2个GPU\n",
    "    # mixed_precision=\"fp16\",  # 使用混合精度训练\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6269030,
     "sourceId": 10768694,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 164048,
     "modelInstanceId": 141469,
     "sourceId": 166258,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
