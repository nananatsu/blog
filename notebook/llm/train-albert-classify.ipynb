{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-25T02:04:21.937952Z",
     "iopub.status.busy": "2025-06-25T02:04:21.937607Z",
     "iopub.status.idle": "2025-06-25T02:04:39.537366Z",
     "shell.execute_reply": "2025-06-25T02:04:39.536684Z",
     "shell.execute_reply.started": "2025-06-25T02:04:21.937914Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    BertTokenizer, get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup,\n",
    "    AlbertModel, AlbertConfig, PreTrainedModel, modeling_outputs\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "import optuna\n",
    "import multiprocessing as mp\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import time\n",
    "import math\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T02:04:39.538885Z",
     "iopub.status.busy": "2025-06-25T02:04:39.538312Z",
     "iopub.status.idle": "2025-06-25T02:04:39.543842Z",
     "shell.execute_reply": "2025-06-25T02:04:39.542873Z",
     "shell.execute_reply.started": "2025-06-25T02:04:39.538841Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 设置随机种子以确保结果可重现\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T02:04:39.546616Z",
     "iopub.status.busy": "2025-06-25T02:04:39.546191Z",
     "iopub.status.idle": "2025-06-25T02:04:39.563019Z",
     "shell.execute_reply": "2025-06-25T02:04:39.562342Z",
     "shell.execute_reply.started": "2025-06-25T02:04:39.546580Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 数据增强函数\n",
    "def augment_text(text, label, augment_prob=0.3):\n",
    "    augmented_texts = [text]\n",
    "    augmented_labels = [label]\n",
    "\n",
    "    # 对少数类别进行更多增强\n",
    "    if label in [3, 4]: \n",
    "        augment_prob *= 2\n",
    "\n",
    "    # 随机插入空格\n",
    "    if random.random() < augment_prob:\n",
    "        words = text.split()\n",
    "        if len(words) > 1:\n",
    "            idx = random.randint(0, len(words))\n",
    "            words.insert(idx, ' ')\n",
    "            augmented_texts.append(' '.join(words))\n",
    "            augmented_labels.append(label)\n",
    "\n",
    "    # 随机删除字符（\n",
    "    if label == 4 and random.random() < augment_prob and len(text) > 5:\n",
    "        idx = random.randint(0, len(text)-1)\n",
    "        new_text = text[:idx] + text[idx+1:]\n",
    "        augmented_texts.append(new_text)\n",
    "        augmented_labels.append(label)\n",
    "\n",
    "    # 随机替换标点符号\n",
    "    if label == 4 and random.random() < augment_prob:\n",
    "        punctuations = ['!', '?', '.', ',', ';', ':', '/', '\\\\', '|']\n",
    "        new_text = text\n",
    "        for _ in range(random.randint(1, 3)):\n",
    "            if len(new_text) > 0:\n",
    "                idx = random.randint(0, len(new_text)-1)\n",
    "                new_text = new_text[:idx] + random.choice(punctuations) + new_text[idx+1:]\n",
    "        augmented_texts.append(new_text)\n",
    "        augmented_labels.append(label)\n",
    "\n",
    "    return augmented_texts, augmented_labels\n",
    "\n",
    "# 定义数据集类\n",
    "class LanguageDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # 对文本进行编码\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T02:04:39.564281Z",
     "iopub.status.busy": "2025-06-25T02:04:39.564006Z",
     "iopub.status.idle": "2025-06-25T02:04:39.579078Z",
     "shell.execute_reply": "2025-06-25T02:04:39.578408Z",
     "shell.execute_reply.started": "2025-06-25T02:04:39.564258Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_dataset(file_path):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    label_map = {'chs': 0, 'cht': 1, 'en': 2, 'sybol': 3, 'error': 4}\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # 处理训练集数据\n",
    "    for line in data['train']:\n",
    "        try:\n",
    "            text, label = line.strip().split('\\t')\n",
    "            if label in label_map:\n",
    "                texts.append(text)\n",
    "                labels.append(label_map[label])\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing: {line}:  {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # 处理验证集数据\n",
    "    val_texts = []\n",
    "    val_labels = []\n",
    "    for line in data['validation']:\n",
    "        try:\n",
    "            text, label = line.strip().split('\\t')\n",
    "            if label in label_map:\n",
    "                val_texts.append(text)\n",
    "                val_labels.append(label_map[label])\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing: {line}:  {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return texts, labels, val_texts, val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T02:04:39.580308Z",
     "iopub.status.busy": "2025-06-25T02:04:39.579974Z",
     "iopub.status.idle": "2025-06-25T02:04:39.596741Z",
     "shell.execute_reply": "2025-06-25T02:04:39.596015Z",
     "shell.execute_reply.started": "2025-06-25T02:04:39.580251Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1.0, gamma=2.0, class_weights=None, label_smoothing=0.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.class_weights = class_weights\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # 应用标签平滑\n",
    "        if self.label_smoothing > 0:\n",
    "            num_classes = inputs.size(-1)\n",
    "            targets_one_hot = F.one_hot(targets, num_classes).float()\n",
    "            targets_one_hot = targets_one_hot * (1 - self.label_smoothing) + \\\n",
    "                             self.label_smoothing / num_classes\n",
    "            ce_loss = F.cross_entropy(inputs, targets_one_hot, weight=self.class_weights, reduction='none')\n",
    "        else:\n",
    "            ce_loss = F.cross_entropy(inputs, targets, weight=self.class_weights, reduction='none')\n",
    "\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        return focal_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T02:04:39.598050Z",
     "iopub.status.busy": "2025-06-25T02:04:39.597810Z",
     "iopub.status.idle": "2025-06-25T02:04:39.619781Z",
     "shell.execute_reply": "2025-06-25T02:04:39.619088Z",
     "shell.execute_reply.started": "2025-06-25T02:04:39.598020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AlbertClassifier(PreTrainedModel):\n",
    "    config_class = AlbertConfig\n",
    "\n",
    "    def __init__(self, config, dropout_rate=0.2):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "\n",
    "        self.albert = AlbertModel(config)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "\n",
    "            nn.Linear(128, config.num_labels)\n",
    "        )\n",
    "\n",
    "        # 添加注意力池化层\n",
    "        self.attention_pool = nn.MultiheadAttention(\n",
    "            embed_dim=config.hidden_size,\n",
    "            num_heads=8,\n",
    "            dropout=dropout_rate,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # 初始化权重\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
    "        outputs = self.albert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        batch_size, seq_len, hidden_size = sequence_output.shape\n",
    "        \n",
    "        query = sequence_output[:, :1, :]  # [CLS]作为查询\n",
    "        key = sequence_output\n",
    "        value = sequence_output\n",
    "        \n",
    "        # 拆分多头\n",
    "        query = query.view(batch_size, 1, 8, hidden_size // 8).transpose(1, 2)\n",
    "        key = key.view(batch_size, seq_len, 8, hidden_size // 8).transpose(1, 2)\n",
    "        value = value.view(batch_size, seq_len, 8, hidden_size // 8).transpose(1, 2)\n",
    "        \n",
    "        # 计算注意力分数\n",
    "        attn_scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(hidden_size // 8)\n",
    "        \n",
    "        # 应用注意力掩码\n",
    "        if attention_mask is not None:\n",
    "            attn_mask = attention_mask.unsqueeze(1).unsqueeze(1).bool()\n",
    "            attn_scores = attn_scores.masked_fill(~attn_mask, float('-inf'))\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, value)\n",
    "        \n",
    "        # 合并多头\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, 1, hidden_size\n",
    "        )\n",
    "        \n",
    "        pooled_output = attn_output.squeeze(1)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return modeling_outputs.SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T02:04:39.621142Z",
     "iopub.status.busy": "2025-06-25T02:04:39.620908Z",
     "iopub.status.idle": "2025-06-25T02:04:39.658186Z",
     "shell.execute_reply": "2025-06-25T02:04:39.657457Z",
     "shell.execute_reply.started": "2025-06-25T02:04:39.621119Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_class_weights(labels, device):\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(labels),\n",
    "        y=labels\n",
    "    )\n",
    "    return torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "def detailed_evaluation(model, val_loader, device, class_weights=None):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    all_losses = []\n",
    "\n",
    "    # 使用Focal Loss进行评估\n",
    "    criterion = FocalLoss(gamma=2.0, class_weights=class_weights, label_smoothing=0.1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"评估中\"):\n",
    "            try:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "\n",
    "                # 计算损失\n",
    "                loss = criterion(outputs.logits, labels)\n",
    "                all_losses.append(loss.item())\n",
    "\n",
    "                # 获取预测和概率\n",
    "                probs = F.softmax(outputs.logits, dim=1)\n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "            except Exception as e:\n",
    "                print(f\"评估错误: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    # 计算各种指标\n",
    "    f1_weighted = f1_score(all_labels, all_preds, average='weighted')\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "    precision, recall, f1_per_class, support = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average=None\n",
    "    )\n",
    "\n",
    "    # 计算每个类别的置信度分布\n",
    "    label_names = ['chs', 'cht', 'en', 'symbol', 'error']\n",
    "    confidence_stats = {}\n",
    "\n",
    "    for i, label_name in enumerate(label_names):\n",
    "        mask = np.array(all_labels) == i\n",
    "        if mask.sum() > 0:\n",
    "            confidence = np.array(all_probs)[mask, i]\n",
    "            confidence_stats[label_name] = {\n",
    "                'mean_confidence': confidence.mean(),\n",
    "                'std_confidence': confidence.std(),\n",
    "                'min_confidence': confidence.min(),\n",
    "                'max_confidence': confidence.max(),\n",
    "                'precision': precision[i],\n",
    "                'recall': recall[i],\n",
    "                'f1': f1_per_class[i],\n",
    "                'support': support[i]\n",
    "            }\n",
    "\n",
    "    # 打印详细报告\n",
    "    print(f\"加权F1分数: {f1_weighted:.4f}\")\n",
    "    print(f\"宏平均F1分数: {f1_macro:.4f}\")\n",
    "    print(f\"平均损失: {np.mean(all_losses):.4f}\")\n",
    "    print(f\"{'类别':<15} {'精确率':<8} {'召回率':<8} {'F1分数':<8} {'支持数':<8} {'平均置信度':<10}\")\n",
    "\n",
    "    for label_name, stats in confidence_stats.items():\n",
    "        print(f\"{label_name:<15} {stats['precision']:<8.3f} {stats['recall']:<8.3f} \"\n",
    "              f\"{stats['f1']:<8.3f} {stats['support']:<8} {stats['mean_confidence']:<10.3f}\")\n",
    "\n",
    "    print(\"\\n混淆矩阵:\")\n",
    "    print(confusion_matrix(all_labels, all_preds))\n",
    "\n",
    "    print(\"\\n分类报告:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=label_names))\n",
    "\n",
    "    return f1_weighted, f1_macro, confidence_stats\n",
    "\n",
    "def evaluate_model(model, val_loader, criterion, device, return_predictions=False):\n",
    "    \"\"\"简化的评估函数，用于训练过程中的快速评估\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"评估中\", leave=False):\n",
    "            try:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "\n",
    "                loss = criterion(outputs.logits, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs.logits, 1)\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "            except Exception as e:\n",
    "                print(f\"评估错误: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    # 计算F1分数\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    avg_loss = total_loss / len(val_loader) if len(val_loader) > 0 else 0\n",
    "\n",
    "    if return_predictions:\n",
    "        return avg_loss, f1, all_preds, all_labels\n",
    "    return avg_loss, f1\n",
    "\n",
    "def progressive_training(model, train_loader, val_loader, device, config):\n",
    "    \"\"\"渐进式训练策略：先训练分类头，再微调整个模型\"\"\"\n",
    "    # 第一阶段：冻结预训练层，只训练分类头\n",
    "    print(\"\\n第一阶段：训练分类头（冻结ALBERT层）\")\n",
    "    for param in model.albert.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # 只训练分类头，使用较大学习率\n",
    "    stage1_results = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        device=device,\n",
    "        num_epochs=5,\n",
    "        learning_rate=1e-3,\n",
    "        warmup_ratio=0.1,\n",
    "        scheduler_type='linear',\n",
    "        accumulation_steps=config.get('accumulation_steps', 1),\n",
    "        max_patience=3,\n",
    "        min_delta=1e-4,\n",
    "        use_focal_loss=True,\n",
    "        stage_name=\"分类头训练\"\n",
    "    )\n",
    "\n",
    "    # 第二阶段：解冻所有层，使用较小学习率微调\n",
    "    print(\"\\n第二阶段：微调整个模型（解冻所有层）\")\n",
    "    for param in model.albert.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # 使用分层学习率\n",
    "    stage2_results = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        device=device,\n",
    "        num_epochs=config.get('num_epochs', 20),\n",
    "        learning_rate=config.get('learning_rate', 2e-5),\n",
    "        warmup_ratio=config.get('warmup_ratio', 0.15),\n",
    "        scheduler_type=config.get('scheduler_type', 'cosine'),\n",
    "        accumulation_steps=config.get('accumulation_steps', 1),\n",
    "        max_patience=config.get('max_patience', 5),\n",
    "        min_delta=config.get('min_delta', 1e-4),\n",
    "        use_focal_loss=True,\n",
    "        use_layered_lr=True,\n",
    "        stage_name=\"全模型微调\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'stage1': stage1_results,\n",
    "        'stage2': stage2_results,\n",
    "        'best_f1': stage2_results['best_f1'],\n",
    "        'best_epoch': stage2_results['best_epoch']\n",
    "    }\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device,\n",
    "                         num_epochs=20, learning_rate=2e-5, warmup_ratio=0.1,\n",
    "                         scheduler_type='cosine', accumulation_steps=1,\n",
    "                         max_patience=5, min_delta=1e-4, use_focal_loss=True,\n",
    "                         use_layered_lr=False, stage_name=\"训练\"):\n",
    "    print(f\"\\n开始{stage_name}阶段...\")\n",
    "    # 分层学习率配置\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "    if use_layered_lr:\n",
    "        # 为不同层设置不同的学习率\n",
    "        optimizer_grouped_parameters = [\n",
    "            # ALBERT层使用较小学习率\n",
    "            {\n",
    "                'params': [p for n, p in model.albert.named_parameters()\n",
    "                          if not any(nd in n for nd in no_decay)],\n",
    "                'weight_decay': 0.01,\n",
    "                'lr': learning_rate * 0.1  # ALBERT层使用1/10的学习率\n",
    "            },\n",
    "            {\n",
    "                'params': [p for n, p in model.albert.named_parameters()\n",
    "                          if any(nd in n for nd in no_decay)],\n",
    "                'weight_decay': 0.0,\n",
    "                'lr': learning_rate * 0.1\n",
    "            },\n",
    "            # 分类头使用正常学习率\n",
    "            {\n",
    "                'params': [p for n, p in model.classifier.named_parameters()\n",
    "                          if not any(nd in n for nd in no_decay)],\n",
    "                'weight_decay': 0.01,\n",
    "                'lr': learning_rate\n",
    "            },\n",
    "            {\n",
    "                'params': [p for n, p in model.classifier.named_parameters()\n",
    "                          if any(nd in n for nd in no_decay)],\n",
    "                'weight_decay': 0.0,\n",
    "                'lr': learning_rate\n",
    "            },\n",
    "            # 注意力池化层\n",
    "            {\n",
    "                'params': [p for n, p in model.attention_pool.named_parameters()\n",
    "                          if not any(nd in n for nd in no_decay)],\n",
    "                'weight_decay': 0.01,\n",
    "                'lr': learning_rate\n",
    "            },\n",
    "            {\n",
    "                'params': [p for n, p in model.attention_pool.named_parameters()\n",
    "                          if any(nd in n for nd in no_decay)],\n",
    "                'weight_decay': 0.0,\n",
    "                'lr': learning_rate\n",
    "            }\n",
    "        ]\n",
    "    else:\n",
    "        # 标准配置\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                'params': [p for n, p in model.named_parameters()\n",
    "                          if not any(nd in n for nd in no_decay)],\n",
    "                'weight_decay': 0.01,\n",
    "            },\n",
    "            {\n",
    "                'params': [p for n, p in model.named_parameters()\n",
    "                          if any(nd in n for nd in no_decay)],\n",
    "                'weight_decay': 0.0,\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=1e-8)\n",
    "\n",
    "    # 计算总训练步数\n",
    "    num_training_steps = len(train_loader) * num_epochs // accumulation_steps\n",
    "    num_warmup_steps = int(num_training_steps * warmup_ratio)\n",
    "\n",
    "    # 学习率调度器\n",
    "    if scheduler_type == 'cosine':\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "    else:\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "\n",
    "    # 获取类别权重\n",
    "    all_labels = []\n",
    "    for batch in train_loader:\n",
    "        all_labels.extend(batch['label'].numpy())\n",
    "    class_weights = get_class_weights(all_labels, device)\n",
    "\n",
    "    # 使用Focal Loss或标准CrossEntropyLoss\n",
    "    if use_focal_loss:\n",
    "        criterion = FocalLoss(\n",
    "            gamma=2.0,\n",
    "            class_weights=class_weights,\n",
    "            label_smoothing=0.1\n",
    "        )\n",
    "        print(f\"使用Focal Loss (gamma=2.0, label_smoothing=0.1)\")\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        print(f\"使用标准CrossEntropyLoss\")\n",
    "\n",
    "    # 训练状态跟踪\n",
    "    best_f1 = 0.0\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    # 历史记录\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    val_f1_history = []\n",
    "    lr_history = []\n",
    "\n",
    "    print(f\"开始训练，总共 {num_epochs} 个epoch\")\n",
    "    print(f\"训练步数: {num_training_steps}, 预热步数: {num_warmup_steps}\")\n",
    "    print(f\"梯度累积步数: {accumulation_steps}\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}')\n",
    "\n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            try:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                loss = outputs.loss / accumulation_steps  # 梯度累积\n",
    "                loss.backward()\n",
    "\n",
    "                total_loss += loss.item() * accumulation_steps\n",
    "\n",
    "                # 梯度累积\n",
    "                if (step + 1) % accumulation_steps == 0:\n",
    "                    # 梯度裁剪\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                # 更新进度条\n",
    "                current_lr = scheduler.get_last_lr()[0] if scheduler.get_last_lr() else learning_rate\n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': f'{loss.item() * accumulation_steps:.4f}',\n",
    "                    'lr': f'{current_lr:.2e}'\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in training batch: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        # 记录训练损失\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        train_loss_history.append(avg_train_loss)\n",
    "        lr_history.append(scheduler.get_last_lr()[0] if scheduler.get_last_lr() else learning_rate)\n",
    "\n",
    "        # 验证阶段\n",
    "        val_loss, val_f1 = evaluate_model(model, val_loader, criterion, device)\n",
    "        val_loss_history.append(val_loss)\n",
    "        val_f1_history.append(val_f1)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}:')\n",
    "        print(f'训练损失: {avg_train_loss:.4f}')\n",
    "        print(f'验证损失: {val_loss:.4f}')\n",
    "        print(f'验证F1: {val_f1:.4f}')\n",
    "        print(f'学习率: {lr_history[-1]:.2e}')\n",
    "\n",
    "        # 保存最佳模型（基于F1分数）\n",
    "        improved = False\n",
    "        if val_f1 > best_f1 + min_delta:\n",
    "            best_f1 = val_f1\n",
    "            best_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            patience_counter = 0\n",
    "            improved = True\n",
    "\n",
    "            # 保存模型\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_f1': best_f1,\n",
    "                'best_loss': best_loss,\n",
    "                'epoch': epoch,\n",
    "                'train_loss_history': train_loss_history,\n",
    "                'val_loss_history': val_loss_history,\n",
    "                'val_f1_history': val_f1_history,\n",
    "                'lr_history': lr_history\n",
    "            }, 'best_classifier_optimized.pt')\n",
    "            print(f'新的最佳模型已保存 (F1: {best_f1:.4f})')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f'未改善 (耐心计数: {patience_counter}/{max_patience})')\n",
    "\n",
    "        # 早停检查\n",
    "        if patience_counter >= max_patience:\n",
    "            print(f'早停触发！在第 {epoch + 1} 个epoch停止训练')\n",
    "            print(f'最佳模型在第 {best_epoch + 1} 个epoch，F1分数: {best_f1:.4f}')\n",
    "            break\n",
    "\n",
    "    # 加载最佳模型\n",
    "    print('训练完成，加载最佳模型...')\n",
    "    checkpoint = torch.load('best_classifier_optimized.pt', weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    # 最终评估\n",
    "    print('\\n使用最佳模型进行最终评估:')\n",
    "    final_val_loss, final_val_f1 = evaluate_model(model, val_loader, criterion, device)\n",
    "    print(f'最终验证损失: {final_val_loss:.4f}')\n",
    "    print(f'最终验证F1: {final_val_f1:.4f}')\n",
    "\n",
    "    return {\n",
    "        'best_f1': best_f1,\n",
    "        'best_loss': best_loss,\n",
    "        'best_epoch': best_epoch,\n",
    "        'train_loss_history': train_loss_history,\n",
    "        'val_loss_history': val_loss_history,\n",
    "        'val_f1_history': val_f1_history,\n",
    "        'lr_history': lr_history\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T02:04:39.660843Z",
     "iopub.status.busy": "2025-06-25T02:04:39.660587Z",
     "iopub.status.idle": "2025-06-25T02:04:39.693941Z",
     "shell.execute_reply": "2025-06-25T02:04:39.693252Z",
     "shell.execute_reply.started": "2025-06-25T02:04:39.660820Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def hyperparameter_search(train_texts, train_labels, val_texts, val_labels,\n",
    "                         tokenizer, model_config, device, n_trials=20, n_jobs=2):\n",
    "    \"\"\"兼容性包装函数，调用并发版本的超参数搜索\"\"\"\n",
    "    return hyperparameter_search_parallel(\n",
    "        train_texts, train_labels, val_texts, val_labels,\n",
    "        tokenizer, model_config, n_trials, n_jobs=n_jobs, use_gpu_parallel=True\n",
    "    )\n",
    "\n",
    "def train_model_fast(model, train_loader, val_loader, device, config, trial_number=0, show_progress=True):\n",
    "    \"\"\"快速训练函数，专门用于超参数搜索，显示详细进度\"\"\"\n",
    "    \n",
    "    if show_progress:\n",
    "        print(f\"Trial {trial_number} - 开始训练\")\n",
    "        print(f\"超参数配置:\")\n",
    "        for key, value in config.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "        print(f\"设备: {device}\")\n",
    "        print(f\"训练批次数: {len(train_loader)}\")\n",
    "        print(f\"验证批次数: {len(val_loader)}\")\n",
    "    \n",
    "    # 优化器\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=0.01)\n",
    "    \n",
    "    # 学习率调度器\n",
    "    num_training_steps = len(train_loader) * config['num_epochs']\n",
    "    num_warmup_steps = int(num_training_steps * config['warmup_ratio'])\n",
    "    \n",
    "    if config['scheduler_type'] == 'cosine':\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps\n",
    "        )\n",
    "    else:\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps\n",
    "        )\n",
    "    \n",
    "    # 损失函数\n",
    "    all_labels = []\n",
    "    for batch in train_loader:\n",
    "        all_labels.extend(batch['label'].numpy())\n",
    "    class_weights = get_class_weights(all_labels, device)\n",
    "    \n",
    "    criterion = FocalLoss(\n",
    "        gamma=config.get('focal_gamma', 2.0), \n",
    "        class_weights=class_weights, \n",
    "        label_smoothing=config.get('label_smoothing', 0.1)\n",
    "    )\n",
    "    \n",
    "    best_f1 = 0.0\n",
    "    patience_counter = 0\n",
    "    max_patience = 2\n",
    "    \n",
    "    if show_progress:\n",
    "        print(f\"\\n开始训练 {config['num_epochs']} 个epochs...\")\n",
    "    \n",
    "    for epoch in range(config['num_epochs']):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            try:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = criterion(outputs.logits, labels)\n",
    "                loss.backward()\n",
    "                \n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                if show_progress:\n",
    "                    print(f\"训练批次错误: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_preds, val_labels = [], []\n",
    "        val_loss = 0\n",
    "        val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                try:\n",
    "                    input_ids = batch['input_ids'].to(device)\n",
    "                    attention_mask = batch['attention_mask'].to(device)\n",
    "                    labels = batch['label'].to(device)\n",
    "                    \n",
    "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                    loss = criterion(outputs.logits, labels)\n",
    "                    preds = torch.argmax(outputs.logits, dim=1)\n",
    "                    \n",
    "                    val_preds.extend(preds.cpu().numpy())\n",
    "                    val_labels.extend(labels.cpu().numpy())\n",
    "                    val_loss += loss.item()\n",
    "                    val_batches += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    if show_progress:\n",
    "                        print(f\"验证批次错误: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        # 计算指标\n",
    "        if len(val_preds) > 0:\n",
    "            f1 = f1_score(val_labels, val_preds, average='weighted')\n",
    "        else:\n",
    "            f1 = 0.0\n",
    "        \n",
    "        avg_train_loss = total_loss / max(num_batches, 1)\n",
    "        avg_val_loss = val_loss / max(val_batches, 1)\n",
    "        \n",
    "        if show_progress:\n",
    "            print(f\"Trial {trial_number} - Epoch {epoch+1}/{config['num_epochs']}:\")\n",
    "            print(f\"训练损失: {avg_train_loss:.4f}\")\n",
    "            print(f\"验证损失: {avg_val_loss:.4f}\")\n",
    "            print(f\"验证F1: {f1:.4f}\")\n",
    "            print(f\"当前最佳F1: {best_f1:.4f}\")\n",
    "            print(f\"学习率: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "        \n",
    "        # 早停检查\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            patience_counter = 0\n",
    "            if show_progress:\n",
    "                print(f\"新的最佳F1分数!\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if show_progress:\n",
    "                print(f\"耐心计数: {patience_counter}/{max_patience}\")\n",
    "        \n",
    "        if patience_counter >= max_patience:\n",
    "            if show_progress:\n",
    "                print(f\"早停触发，停止训练\")\n",
    "            break\n",
    "    \n",
    "    if show_progress:\n",
    "        print(f\"\\nTrial {trial_number} 完成!\")\n",
    "        print(f\"最终F1分数: {best_f1:.4f}\")\n",
    "        print(f\"实际训练epochs: {epoch+1}\")\n",
    "    \n",
    "    return best_f1\n",
    "\n",
    "def objective_function(trial, train_texts, train_labels, val_texts, val_labels,\n",
    "                      tokenizer, model_config, use_gpu_parallel=True, show_progress=True):\n",
    "    try:\n",
    "        # 设备选择\n",
    "        if use_gpu_parallel and torch.cuda.is_available():\n",
    "            gpu_count = torch.cuda.device_count()\n",
    "            gpu_id = trial.number % gpu_count\n",
    "            device = torch.device(f'cuda:{gpu_id}')\n",
    "        else:\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # 超参数建议\n",
    "        config = {\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-5, 5e-5, log=True),\n",
    "            'batch_size': trial.suggest_categorical('batch_size', [256, 512]),\n",
    "            'dropout_rate': trial.suggest_float('dropout_rate', 0.1, 0.3),\n",
    "            'warmup_ratio': trial.suggest_float('warmup_ratio', 0.05, 0.2),\n",
    "            'num_epochs': trial.suggest_int('num_epochs', 6, 12),  # 快速搜索\n",
    "            'scheduler_type': trial.suggest_categorical('scheduler_type', ['linear', 'cosine']),\n",
    "            'focal_gamma': trial.suggest_float('focal_gamma', 1.0, 3.0),\n",
    "            'label_smoothing': trial.suggest_float('label_smoothing', 0.0, 0.2)\n",
    "        }\n",
    "\n",
    "        if show_progress:\n",
    "            print(f\"Trial {trial.number} 超参数:\")\n",
    "            for key, value in config.items():\n",
    "                print(f\"{key}: {value}\")\n",
    "\n",
    "        # 创建模型\n",
    "        model_config_copy = copy.deepcopy(model_config)\n",
    "        model = AlbertClassifier(model_config_copy, dropout_rate=config['dropout_rate'])\n",
    "        model = model.to(device)\n",
    "        model.albert = AlbertModel.from_pretrained(\"voidful/albert_chinese_tiny\", config=model_config_copy).to(device)\n",
    "\n",
    "        # 创建数据集\n",
    "        train_dataset = LanguageDataset(train_texts, train_labels, tokenizer)\n",
    "        val_dataset = LanguageDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=config['batch_size'])\n",
    "\n",
    "        # 训练\n",
    "        start_time = time.time()\n",
    "        best_f1 = train_model_fast(model, train_loader, val_loader, device, config,\n",
    "                                  trial_number=trial.number, show_progress=show_progress)\n",
    "        training_time = time.time() - start_time\n",
    "\n",
    "        if show_progress:\n",
    "            print(f\"\\nTrial {trial.number} 训练时间: {training_time:.1f}秒\")\n",
    "            print(f\"Trial {trial.number} 最终F1分数: {best_f1:.4f}\")\n",
    "\n",
    "        # 清理内存\n",
    "        del model\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return best_f1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Trial {trial.number} 失败: {str(e)}\")\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        return 0.0\n",
    "\n",
    "def hyperparameter_search_parallel(train_texts, train_labels, val_texts, val_labels,\n",
    "        tokenizer, model_config, n_trials=30, n_jobs=4, use_gpu_parallel=True, show_progress=True):\n",
    "    \"\"\"并发超参数搜索主函数\"\"\"\n",
    "    print(\"并发超参数搜索\")\n",
    "    # 设置随机种子\n",
    "    set_seed(42)\n",
    "\n",
    "    # 检查资源\n",
    "    max_workers = min(n_jobs, mp.cpu_count())\n",
    "    print(f\"CPU核心数: {mp.cpu_count()}\")\n",
    "    print(f\"使用并发数: {max_workers}\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"可用GPU数: {torch.cuda.device_count()}\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"GPU并行: {'启用' if use_gpu_parallel else '禁用'}\")\n",
    "    else:\n",
    "        print(\"GPU: 不可用\")\n",
    "        use_gpu_parallel = False\n",
    "\n",
    "    # 创建objective函数\n",
    "    def objective(trial):\n",
    "        return objective_function(\n",
    "            trial, train_texts, train_labels, val_texts, val_labels,\n",
    "            tokenizer, model_config, use_gpu_parallel, show_progress\n",
    "        )\n",
    "\n",
    "    # 创建study\n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        sampler=optuna.samplers.TPESampler(n_startup_trials=8),\n",
    "        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=2)\n",
    "    )\n",
    "\n",
    "    print(f\"\\n开始搜索 {n_trials} 个试验...\")\n",
    "    print(f\"并发进程数: {max_workers}\")\n",
    "    print(f\"目标: 最大化F1分数\")\n",
    "\n",
    "    # 执行搜索\n",
    "    start_time = time.time()\n",
    "    study.optimize(objective, n_trials=n_trials, n_jobs=max_workers)\n",
    "    search_time = time.time() - start_time\n",
    "\n",
    "    # 结果分析\n",
    "    print(\"搜索结果\")\n",
    "    print(f\"最佳F1分数: {study.best_value:.4f}\")\n",
    "    print(f\"总搜索时间: {search_time:.1f}秒 ({search_time/60:.1f}分钟)\")\n",
    "    print(f\"平均每个trial: {search_time/len(study.trials):.1f}秒\")\n",
    "\n",
    "    print(f\"\\n最佳超参数:\")\n",
    "    for key, value in study.best_params.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    print(f\"\\n搜索统计:\")\n",
    "    completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "    pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "    failed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.FAIL]\n",
    "\n",
    "    print(f\"完成试验数: {len(completed_trials)}\")\n",
    "    print(f\"剪枝试验数: {len(pruned_trials)}\")\n",
    "    print(f\"失败试验数: {len(failed_trials)}\")\n",
    "\n",
    "    if len(completed_trials) > 1:\n",
    "        f1_scores = [t.value for t in completed_trials if t.value is not None]\n",
    "        print(f\"F1分数范围: {min(f1_scores):.4f} - {max(f1_scores):.4f}\")\n",
    "        print(f\"F1分数均值: {np.mean(f1_scores):.4f}\")\n",
    "        print(f\"F1分数标准差: {np.std(f1_scores):.4f}\")\n",
    "\n",
    "    # 保存结果\n",
    "    print({\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'best_params': study.best_params,\n",
    "        'best_value': study.best_value,\n",
    "        'search_time_seconds': search_time,\n",
    "        'n_trials': n_trials,\n",
    "        'n_jobs': max_workers,\n",
    "        'use_gpu_parallel': use_gpu_parallel,\n",
    "        'statistics': {\n",
    "            'completed': len(completed_trials),\n",
    "            'pruned': len(pruned_trials),\n",
    "            'failed': len(failed_trials),\n",
    "            'f1_scores': f1_scores if len(completed_trials) > 1 else []\n",
    "        },\n",
    "        'data_info': {\n",
    "            'train_samples': len(train_texts),\n",
    "            'val_samples': len(val_texts),\n",
    "        }\n",
    "    })\n",
    "\n",
    "    return study.best_params, study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T02:04:39.695313Z",
     "iopub.status.busy": "2025-06-25T02:04:39.695046Z",
     "iopub.status.idle": "2025-06-25T02:04:41.517667Z",
     "shell.execute_reply": "2025-06-25T02:04:41.516786Z",
     "shell.execute_reply.started": "2025-06-25T02:04:39.695257Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 设置随机种子\n",
    "set_seed(42)\n",
    "# 训练配置\n",
    "config = {\n",
    "    'model_name': \"voidful/albert_chinese_tiny\",\n",
    "    'max_length': 128,\n",
    "    'batch_size': 256,\n",
    "    'num_epochs': 25,\n",
    "    'learning_rate': 4.8402352659412e-05,  # 提高学习率\n",
    "    'warmup_ratio': 0.07065493676186665,   # 增加warmup比例\n",
    "    'scheduler_type': 'cosine',  # 'linear' or 'cosine'\n",
    "    'accumulation_steps': 1,\n",
    "    'max_patience': 5,\n",
    "    'min_delta': 1e-4,\n",
    "    'dropout_rate': 0.17387168694938498,    # 降低dropout\n",
    "    'train_ratio': 0.8,\n",
    "    'val_ratio': 0.1,\n",
    "    'test_ratio': 0.1,\n",
    "    'use_weighted_sampler': True,\n",
    "    'data_augmentation': True,\n",
    "    'use_hyperparameter_search': False,  # 是否进行超参数搜索\n",
    "    'use_progressive_training': True,    # 是否使用渐进式训练\n",
    "    'hyperparameter_trials': 15         # 超参数搜索试验次数\n",
    "}\n",
    "\n",
    "# 设备配置\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 加载tokenizer和模型配置\n",
    "print(f\"\\n加载模型: {config['model_name']}\")\n",
    "tokenizer = BertTokenizer.from_pretrained(config['model_name'])\n",
    "\n",
    "model_config = AlbertConfig.from_pretrained(\n",
    "    config['model_name'],\n",
    "    num_labels=5,\n",
    "    id2label={0: \"chs\", 1: \"cht\", 2: \"en\", 3: \"symbol\", 4: \"error\"},\n",
    "    label2id={\"chs\": 0, \"cht\": 1, \"en\": 2, \"symbol\": 3, \"error\": 4}\n",
    ")\n",
    "\n",
    "# 创建模型\n",
    "model = AlbertClassifier(model_config, dropout_rate=config['dropout_rate'])\n",
    "model = model.to(device)\n",
    "model.albert = AlbertModel.from_pretrained(config['model_name'], config=model_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T02:04:41.519357Z",
     "iopub.status.busy": "2025-06-25T02:04:41.518963Z",
     "iopub.status.idle": "2025-06-25T02:04:41.620659Z",
     "shell.execute_reply": "2025-06-25T02:04:41.619753Z",
     "shell.execute_reply.started": "2025-06-25T02:04:41.519317Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 加载和分割数据\n",
    "print(\"\\n加载数据...\")\n",
    "# 加载划分好的数据集\n",
    "train_texts, train_labels, val_texts, val_labels = load_dataset('./dataset.json')\n",
    "\n",
    "# 创建数据集\n",
    "train_dataset = LanguageDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = LanguageDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T02:04:41.622038Z",
     "iopub.status.busy": "2025-06-25T02:04:41.621768Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 超参数搜索（可选）\n",
    "if config['use_hyperparameter_search']:\n",
    "    print(\"执行超参数搜索\")\n",
    "    best_params, best_score = hyperparameter_search(\n",
    "        train_texts=train_texts,\n",
    "        train_labels=train_labels,\n",
    "        val_texts=val_texts,\n",
    "        val_labels=val_labels,\n",
    "        tokenizer=tokenizer,\n",
    "        model_config=model_config,\n",
    "        device=device,\n",
    "        n_trials=config['hyperparameter_trials']\n",
    "    )\n",
    "\n",
    "    # 更新配置为最佳参数\n",
    "    for key, value in best_params.items():\n",
    "        if key in config:\n",
    "            config[key] = value\n",
    "\n",
    "    print(f\"\\n使用最佳超参数重新创建模型...\")\n",
    "    # 重新创建模型\n",
    "    model = AlbertClassifier(model_config, dropout_rate=config['dropout_rate'])\n",
    "    model = model.to(device)\n",
    "    model.albert = AlbertModel.from_pretrained(config['model_name'], config=model_config).to(device)\n",
    "\n",
    "# 开始训练\n",
    "print(\"开始主要训练流程\")\n",
    "if config['use_progressive_training']:\n",
    "    # 使用渐进式训练\n",
    "    training_results = progressive_training(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        device=device,\n",
    "        config=config\n",
    "    )\n",
    "else:\n",
    "    # 使用标准训练\n",
    "    training_results = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        device=device,\n",
    "        num_epochs=config['num_epochs'],\n",
    "        learning_rate=config['learning_rate'],\n",
    "        warmup_ratio=config['warmup_ratio'],\n",
    "        scheduler_type=config['scheduler_type'],\n",
    "        accumulation_steps=config['accumulation_steps'],\n",
    "        max_patience=config['max_patience'],\n",
    "        min_delta=config['min_delta'],\n",
    "        use_focal_loss=True,\n",
    "        use_layered_lr=True,\n",
    "        stage_name=\"标准训练\"\n",
    "    )\n",
    "\n",
    "print(\"执行评估\")\n",
    "# 获取类别权重用于评估\n",
    "all_labels = []\n",
    "for batch in train_loader:\n",
    "    all_labels.extend(batch['label'].numpy())\n",
    "class_weights = get_class_weights(all_labels, device)\n",
    "\n",
    "# 执行评估\n",
    "f1_weighted, f1_macro, confidence_stats = detailed_evaluation(\n",
    "    model=model,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    class_weights=class_weights\n",
    ")\n",
    "\n",
    "print(f\"\\n最终训练结果:\")\n",
    "print(f\"最佳F1分数: {training_results['best_f1']:.4f}\")\n",
    "print(f\"最佳epoch: {training_results['best_epoch']}\")\n",
    "print(f\"加权F1分数: {f1_weighted:.4f}\")\n",
    "print(f\"宏平均F1分数: {f1_macro:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_dir='albert_tiny_chinese_classifier'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "# 保存模型和配置\n",
    "model.save_pretrained(output_dir)\n",
    "# 保存分词器\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f\"模型已保存到 {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导出模型为onnx格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optimum[onnxruntime] onnxscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 导出为ONNX格式\n",
    "dummy_input = {\n",
    "    'input_ids': torch.randint(0, tokenizer.vocab_size, (1, 128), dtype=torch.long).to(device),\n",
    "    'attention_mask': torch.ones(1, 128, dtype=torch.long).to(device)\n",
    "}\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (dummy_input['input_ids'], dummy_input['attention_mask']),\n",
    "    output_dir+'/model.onnx',\n",
    "    input_names=['input_ids', 'attention_mask'],\n",
    "    output_names=['logits'],\n",
    "    dynamic_axes={\n",
    "        'input_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
    "        'attention_mask': {0: 'batch_size', 1: 'sequence_length'},\n",
    "        'logits': {0: 'batch_size'}\n",
    "    },\n",
    "    opset_version=14,\n",
    "    do_constant_folding=True,\n",
    "    export_params=True,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "量化onnx模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import  ORTQuantizer\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "import onnxruntime as ort\n",
    "import os\n",
    "\n",
    "# 创建量化器\n",
    "quantizer = ORTQuantizer.from_pretrained(output_dir,file_name='model.onnx')\n",
    "# 配置量化参数\n",
    "qconfig = AutoQuantizationConfig.avx512_vnni(\n",
    "    is_static=False,\n",
    "    per_channel=False\n",
    ")\n",
    "# 量化模型\n",
    "quantizer.quantize(\n",
    "    save_dir=output_dir,\n",
    "    quantization_config=qconfig\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6269030,
     "sourceId": 12274960,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
