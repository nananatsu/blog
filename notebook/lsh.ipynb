{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4+6nBdHTpYuY6hLDxlJQI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nananatsu/blog/blob/master/lsh.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSH(Locality sensitive hashing)，局部敏感哈希，通过最大化哈希冲突将相似的向量散列到相同的桶中，也可以将该计术视为一种降维方法，将高维度向量转为低维度向量，保留向量间的相对距离。\n",
        "\n",
        "LSH有多种版本，使用不同的哈希函数和距离度量，两种流行方案："
      ],
      "metadata": {
        "id": "C16rGltAVSY-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - 最小哈希\n",
        "\n",
        "  MinHash是一种快速估计集合相似度的方法，$h$是将集合$U$的成员映射为不同整数的哈希函数，$perm$是集合$U$元素的随机排列，对于$U$的子集$S$，定义$h_{min}(S)$是集合$S$中使得$h(perm(x))$具有最小值的元素\n",
        "\n",
        "  对于集合$A、B$，假设没有哈希冲突，当且仅当在所有元素$A ∪ B$集合中中具有最小Hash值的元素位于$A \\cap B$集合中时存在$h_{min}(A) = h_{min}(B)$，对应概率便是雅卡尔指数\n",
        "\n",
        "  $P_r[(h_{min}(A) = h_{min}(B))] = \\frac{ \\vert  x \\bigcap y \\vert }{ \\vert  x \\bigcup y \\vert }$\n",
        "\n",
        "  如果$r$是一个随机变量，当$h_{min}(A) = h_{min}(B)$时为1，其他时候为0，则r是集合A、B的雅卡尔指数的一个无偏估计，r总是0或1使得对应方差很高(值偏离均值的程度)，无法单独用来估计雅卡尔指数，MinHash通过相同方式构造多个变量进行平均以减少相应方差。\n",
        "\n",
        "  MinHash的最简方案是使用k个不同的哈希函数，设y是满足$h_{min}(A) = h_{min}(B)$的哈希函数数量（可以视为无偏估计r的和），对应的雅卡尔指数估计为$y/k$，这k个函数的对应的$h_{min}(A)$集合保留了原集合与其他集合的相似度，可以用它来表示原集合。"
      ],
      "metadata": {
        "id": "YfzQZyqfW9PO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - 随机投影\n",
        "\n",
        "  选择一个随机超平面（由法线单位向量r定义）对输入向量进行Hash处理，给定输入向量$v$和一个$r$定义的超平面，使$h(v) = sgn(v ⋅ r) $，$h(v)=±1$取决于v在超平面的哪侧\n",
        "  \n",
        "  对于向量$v,u$，存在$P_r[h(v) = h(u)] = 1 - \\frac{θ(v,u)}{π}$，$\\frac{θ(v,u)}{π}$与$1-cos(θ(v,u))$成正比，即两个向量位于超平面同一侧的概率与它们之间的余弦距离成正比\n",
        "\n",
        "  将出现在超平面负侧的点分配值0，出现在正侧的点数据分配值1，通过将向量与超平面法线向量进行内积运算，可以确定向量位于超平面哪一侧，如果两个向量共享相同方向则其内积为正，如不共享相同方向，则为负，两个向量完全垂直其内积为0，将其与负侧向量分组\n",
        "\n",
        "  单个二进制不能告诉我们关于向量相似性的信息，当我们添加更多超平面时，编码信息量会迅速增加，通过使用这些超平面将向量投影到低维空间，从而生成了新的散列向量"
      ],
      "metadata": {
        "id": "r6fx4XRYilZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "准备数据测试数据"
      ],
      "metadata": {
        "id": "1V5rwYX2zbmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "!mkdir -p mydata\n",
        "!unzip /content/drive/MyDrive/train_data/baike2018qa.zip -d mydata"
      ],
      "metadata": {
        "id": "yT08KbRTW7Jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install numpy pandas scipy sentence-transformers torch faiss-gpu"
      ],
      "metadata": {
        "id": "UuAHhX37eDNz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_json('./mydata/baike_qa_train.json', lines=True, nrows=5000)\n",
        "\n",
        "sentences = data['title']"
      ],
      "metadata": {
        "id": "DjJjGengo_oT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 最小哈希\n",
        "\n",
        "最小哈希版本的LSH分为三步：k-shingling、MinHash、LSH Band\n",
        "\n"
      ],
      "metadata": {
        "id": "WU-Yyc2mlem_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "k-shingling，分割字符串，提取特征向量。\n",
        "  - 分割字符串为短字符串集合，沿着字符串移动一个长度k的窗口，将窗口中字符串写入集合；"
      ],
      "metadata": {
        "id": "0bKmx67qaHYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_shingles(sentence: str, k: int):\n",
        "  # 将字符串分割为k个短字符串集合\n",
        "  shingles = []\n",
        "  length = len(sentence)\n",
        "  if length > k:\n",
        "    for i in range(length - k):\n",
        "        shingles.append(sentence[i:i+k])\n",
        "  else:\n",
        "    shingles.append(sentence)\n",
        "  return set(shingles)\n",
        "\n",
        "def split_shingles_batch(sentences: list[str], k: int):\n",
        "  shingles_list = []\n",
        "  for sentence in sentences:\n",
        "    shingles_list.append(split_shingles(sentence,k))\n",
        "  return shingles_list"
      ],
      "metadata": {
        "id": "_s1ntCwyXcSK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 4\n",
        "shingles_list =split_shingles_batch(sentences, k)\n",
        "\n",
        "shingles_list[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4GroM1RpCt6",
        "outputId": "8c758ad1-1e93-483f-8224-e1b5c9d7b60b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'上为什么',\n",
              " '下的感觉',\n",
              " '为什么没',\n",
              " '么没有头',\n",
              " '人站在地',\n",
              " '什么没有',\n",
              " '在地球上',\n",
              " '地球上为',\n",
              " '头朝下的',\n",
              " '有头朝下',\n",
              " '朝下的感',\n",
              " '没有头朝',\n",
              " '球上为什',\n",
              " '站在地球'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "   \n",
        "  - 合并所有字符串集合，生成一个包含所有词汇的词汇表，再依据词汇表使用one-hot编码将各个字符串集合转换为稀疏向量；\n",
        "    - 为每个字符串集合，创建一个词汇表长度的特征向量，词汇表某个位置的词有出现在字符串集合，将该位置置为1，否则为0。"
      ],
      "metadata": {
        "id": "EM9bJ8fUZbc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def build_vocab(shingle_sets: list):\n",
        "    # 合并所有shingle集合到词汇表\n",
        "    full_set = {item for set_ in shingle_sets for item in set_}\n",
        "    vocab = {}\n",
        "    for i, shingle in enumerate(list(full_set)):\n",
        "        vocab[shingle] = i\n",
        "    return vocab\n",
        "\n",
        "def one_hot(shingles: set, vocab: dict):\n",
        "    # 对短字符串集合进行one-hot编码\n",
        "    vec = np.zeros(len(vocab))\n",
        "    for shingle in shingles:\n",
        "        idx = vocab[shingle]\n",
        "        vec[idx] = 1\n",
        "    \n",
        "    return vec\n",
        "\n",
        "def one_hot_batch(shingles_list: list[set[str]], vocab: dict):\n",
        "  feature_matrix = []\n",
        "  for shingles in shingles_list:\n",
        "    feature_matrix.append(one_hot(shingles,vocab))\n",
        "\n",
        "  return np.stack(feature_matrix)\n"
      ],
      "metadata": {
        "id": "GkK36FFUWctJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = build_vocab(shingles_list)\n",
        "\n",
        "feature_matrix = one_hot_batch(shingles_list, vocab)\n",
        "\n",
        "feature_matrix.shape"
      ],
      "metadata": {
        "id": "K9Sb3ZDael_J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddbefaf9-8abf-4899-af8a-c73ad316c41a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 105391)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_matrix[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XolaEXXVc00-",
        "outputId": "44920615-c876-40b0-ef0f-2cda38ed3a02"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - MinHash 将稀疏向量转换为密集向量（签名）\n",
        "    - 哈希函数是一个随机向量，是词汇表索引的随机排列\n",
        "    - 从1开始取得哈希函数中值对应索引，通过索引访问稀疏向量，找到第1个为1的索引，该索引在哈希函数对应值便是需要的最小哈希\n",
        "    - 遍历生成的哈希函数，将得到的最小哈希组成密集向量（签名）"
      ],
      "metadata": {
        "id": "8ip_AzkhzbNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_minhash_func(vocab_size: int, nbits: int):\n",
        "    hashes = np.zeros((nbits, vocab_size))\n",
        "    for i in range(nbits):\n",
        "      # 在[0,vocab_size)区间生成值随机排列\n",
        "      permutation = np.random.permutation(vocab_size) + 1\n",
        "      # 更新到minhash的第i行\n",
        "      hashes[i,:] = permutation.copy()\n",
        "    return hashes.astype(int)\n",
        "\n",
        "def get_signature(vector: list, minhash_func):\n",
        "    # 找到向量中第所有非0元素位置\n",
        "    idx = np.nonzero(vector)[0].tolist()\n",
        "    # 通过非0元素位置取得哈希函数值\n",
        "    shingles = minhash_func[:, idx]\n",
        "    # 从哈希值中找到每行的最小值，作为签名\n",
        "    signature = np.min(shingles, axis=1)\n",
        "\n",
        "    return signature\n",
        "\n",
        "def get_signature_batch(feature_matrix:list,minhash_func:list):\n",
        "  signatures = []\n",
        "  for feature in feature_matrix:\n",
        "    signature = get_signature(feature, minhash_func)\n",
        "    signatures.append(signature)\n",
        "\n",
        "  return np.stack(signatures)"
      ],
      "metadata": {
        "id": "5XcNtrANPv70"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minhash_func = create_minhash_func(len(vocab), 100)\n",
        "\n",
        "signatures = get_signature_batch(feature_matrix,minhash_func)\n",
        "\n",
        "signatures.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzC4ENTbcwr9",
        "outputId": "fbf0a6d0-06ac-4b4f-ef90-866b6921ab3a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "signatures[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wob3MZ1wdACR",
        "outputId": "e166aad7-c2e9-4d77-c4b8-a2a63da0cf26"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1167,  892,  430, 3694,  309,  412, 1087,  344, 3089, 1946,  436,\n",
              "       2559,  594, 1235, 1042,  112, 1157,  556, 1075,  367,  841, 1082,\n",
              "       4229, 2456, 1750,   74, 1575, 2383, 2018,  786, 3276,  610, 3228,\n",
              "        408,  208,  557,  851,  614, 1594, 1327, 3177,  238,  854,  784,\n",
              "        105, 2940,  265, 4173, 4190,  412, 2397, 1291,  903,  123, 2295,\n",
              "        908,  323, 3946, 1654, 7152, 1843,  600, 5850,  171,  857,  294,\n",
              "       1592,  348, 1235,  219,  758,  268,  601,  386,  815, 2019, 1866,\n",
              "        131, 2700, 1359,  712, 2438,  472, 4114,  996,  464, 1933, 1981,\n",
              "       3083,  654, 3026, 2501,  481, 3804, 1713,  426,  329, 2969,  881,\n",
              "       4350])"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - LSH Band 获取密集向量进行哈希，寻找哈希冲突，从而将向量放入桶\n",
        "    - 如将向量整体进行哈希，很难通过哈希函数来识别向量间的相似性，我们通过将向量分为几个子段（称为Band），哈希向量的每个段独立进行哈希；\n",
        "    - 查询时，两个向量的任意子向量存在冲突，都被视为候选；"
      ],
      "metadata": {
        "id": "53EFRgslaKPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_vec(signature: list,band :int):\n",
        "  # 分割向量为多个子向量\n",
        "  l = len(signature)\n",
        "  assert l % band == 0\n",
        "  r = int(l/band)\n",
        "  subvecs = []\n",
        "  for i in range(0,l,r):\n",
        "    subvecs.append(signature[i:i+r])\n",
        "  return np.stack(subvecs)\n",
        "\n",
        "def lsh_band(signatures:list,band:int):\n",
        "  # 将输入向量哈希到不同桶中\n",
        "  buckets = []\n",
        "  for i in range(band):\n",
        "    buckets.append({})\n",
        "\n",
        "  for i,signature in enumerate(signatures):\n",
        "    subvecs = split_vec(signature, band).astype(str)\n",
        "    for j,subvec in enumerate(subvecs):\n",
        "      subvec = ','.join(subvec)\n",
        "      if subvec not in buckets[j].keys():\n",
        "        buckets[j][subvec] = []\n",
        "      buckets[j][subvec].append(i)\n",
        "\n",
        "  return buckets"
      ],
      "metadata": {
        "id": "RV8-yIe7ektw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "band = 50\n",
        "buckets = lsh_band(signatures,band)\n",
        "\n",
        "len(buckets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hCV9N4l5FPr",
        "outputId": "c73bdd64-89fb-4496-d7d8-89cd2811543e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_canidate(signature, band, buckets):\n",
        "  # 从桶中取得存在哈希冲突的向量位置\n",
        "  candidate = []\n",
        "  subvecs = split_vec(signature,band).astype(str)\n",
        "  for i,subvec in enumerate(subvecs):\n",
        "    subvec = ','.join(subvec)\n",
        "    if subvec in buckets[i].keys():\n",
        "      candidate.extend(buckets[i][subvec])\n",
        "\n",
        "  return set(candidate)\n",
        "\n",
        "def one_hot_for_query(shingles: set, vocab: dict):\n",
        "    vec = np.zeros(len(vocab))\n",
        "    for shingle in shingles:\n",
        "        idx = vocab.get(shingle)\n",
        "        if idx != None:\n",
        "          vec[idx] = 1\n",
        "    return vec"
      ],
      "metadata": {
        "id": "-Fap_VR5oQGs"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial import distance\n",
        "\n",
        "for i in np.random.permutation(len(sentences))[:5]:\n",
        "    query_sentence = sentences[i]\n",
        "\n",
        "    query_vector = one_hot_for_query(split_shingles(query_sentence,k),vocab)\n",
        "    query_signature = get_signature(query_vector, minhash_func)\n",
        "    result_candidate =  get_canidate(query_signature, band, buckets)\n",
        "    print('查询:', query_sentence)\n",
        "    for idx in result_candidate:\n",
        "      print('   L2距离:',distance.euclidean(query_vector,feature_matrix[idx]),'匹配:',sentences[idx] )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-slP4ZjZwZLi",
        "outputId": "e3300557-899b-4522-ddd0-d61bec0072e6"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "查询: 华菱000932成本3.66元，节后该如何操作？继续等待或择机卖 \n",
            "   L2距离: 0.0 匹配: 华菱000932成本3.66元，节后该如何操作？继续等待或择机卖 \n",
            "查询: 输电线路电压偏差过大原因分析我公司有一段66KV输电线路LGJ\n",
            "   L2距离: 0.0 匹配: 输电线路电压偏差过大原因分析我公司有一段66KV输电线路LGJ\n",
            "查询: 诛仙印象第5期的背景音乐哪里有下载的?? \n",
            "   L2距离: 0.0 匹配: 诛仙印象第5期的背景音乐哪里有下载的?? \n",
            "查询: WPS转换为word文档时,数学公式怎么转?以前的一个WPS文档 \n",
            "   L2距离: 0.0 匹配: WPS转换为word文档时,数学公式怎么转?以前的一个WPS文档 \n",
            "查询: QQ车音响如何改装我喜欢听流行音乐.比较轻柔的.请指教:最好是指 \n",
            "   L2距离: 0.0 匹配: QQ车音响如何改装我喜欢听流行音乐.比较轻柔的.请指教:最好是指 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 随机投影\n"
      ],
      "metadata": {
        "id": "YLOawKmi5I7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('DMetaSoul/sbert-chinese-general-v2')\n",
        "sentence_embeddings = model.encode(sentences)"
      ],
      "metadata": {
        "id": "Za-6PTanfBiW"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "按输入向量维度生成随机超平面"
      ],
      "metadata": {
        "id": "-wugEzI3jENm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nbits = 16 #超平面数量\n",
        "d = sentence_embeddings.shape[1]\n",
        "\n",
        "plane_norms = np.random.rand(nbits, d) - .5\n",
        "plane_norms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pPySzjH5UD6",
        "outputId": "6e9167d3-34d2-4dfb-b7e8-3bbbdd234d02"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.18880271, -0.18412385,  0.12060263, ...,  0.17444815,\n",
              "         0.05596546,  0.48311439],\n",
              "       [ 0.07260546, -0.49526173,  0.22410239, ..., -0.23384566,\n",
              "         0.06609219,  0.20360384],\n",
              "       [ 0.0005011 , -0.22629873, -0.05950452, ...,  0.12421859,\n",
              "         0.45558575, -0.25598566],\n",
              "       ...,\n",
              "       [-0.09102182,  0.39235505,  0.17571101, ..., -0.22167728,\n",
              "         0.26765463,  0.32822266],\n",
              "       [-0.15349363, -0.407366  ,  0.40097011, ...,  0.3618219 ,\n",
              "        -0.28554909, -0.27658608],\n",
              "       [-0.02408455,  0.17956788,  0.02613843, ..., -0.1494725 ,\n",
              "         0.22731098,  0.1298718 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "计算输入向量与超平面的内积，当内积大于0结果为1，其他情况为0，生成超平面数量大小的输出向量"
      ],
      "metadata": {
        "id": "RhHr7yTyjN3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dot_vector(vec, plane_norms):\n",
        "  dot = np.dot(vec, plane_norms.T)\n",
        "  dot = dot > 0\n",
        "  dot = dot.astype(int)\n",
        " \n",
        "  return dot"
      ],
      "metadata": {
        "id": "YLgTTaVr7n0j"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dot_arr = []\n",
        "for embedding in sentence_embeddings:\n",
        "  dot_arr.append(create_dot_vector(embedding,plane_norms))\n",
        "\n",
        "dot_arr[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8yG4z3vA0FY",
        "outputId": "7c4c9ef9-22a5-427b-ba58-cc9ac371a2ea"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "将降维后向量作为键计算哈希，将向量分配到不同桶中"
      ],
      "metadata": {
        "id": "ja5NCZV2jodN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hash_buckets = {}\n",
        "\n",
        "for i in range(len(dot_arr)):\n",
        "  key = ''.join(dot_arr[i].astype(str))\n",
        "  if key not in hash_buckets.keys():\n",
        "    hash_buckets[key] = []\n",
        "  hash_buckets[key].append(i)\n",
        "\n",
        "len(hash_buckets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SrLZQ4D8i1Z",
        "outputId": "7a36a0bc-ce0b-42fa-a1b8-6528769a7318"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2459"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "生成降维后向量所有可能形式，即所有可能桶的键"
      ],
      "metadata": {
        "id": "5EhA_3fPsfYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keys = []\n",
        "for i in range(1 << nbits):\n",
        "  b = bin(i)[2:]\n",
        "  # 按超平面数量补0\n",
        "  b = '0' * (nbits - len(b))+b\n",
        "  b = [int(i) for i in b]\n",
        "  keys.append(b)\n",
        "keys = np.stack(keys)\n",
        "keys.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQdsPunYspre",
        "outputId": "186905da-dc19-4503-8edf-74ad478280d0"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(65536, 16)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "计算查询向量与桶可能键间的汉明距离，找到最近的桶，从桶中取出最相似的k个向量"
      ],
      "metadata": {
        "id": "Ymvmgt4Ij2yO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_candidate(dot_vec, k):\n",
        "  # 统计查询向量与所有桶的间的汉明距离并以列表示\n",
        "  haming = np.count_nonzero(dot_vec != keys,axis=1).reshape(-1,1)\n",
        "  # 将距离添加到键矩阵最后一列\n",
        "  haming = np.concatenate((keys, haming), axis=1)\n",
        "  # 将键矩阵行按汉明距离排序\n",
        "  haming =haming[haming[:, -1].argsort()] \n",
        "  # 从桶中取出k个向量的对应位置\n",
        "  vec_ids = []\n",
        "  for row in haming:\n",
        "    key = ''.join(row[:-1].astype(str))\n",
        "    bucket = hash_buckets.get(key)\n",
        "    if bucket is None:\n",
        "      continue\n",
        "    vec_ids.extend(bucket)\n",
        "    if len(vec_ids) > k:\n",
        "      vec_ids = vec_ids[:k]\n",
        "      break\n",
        "  return vec_ids"
      ],
      "metadata": {
        "id": "JY_pqNVh9VOL"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "\n",
        "indexLSH = faiss.IndexLSH(d, nbits)\n",
        "indexLSH.add(sentence_embeddings)"
      ],
      "metadata": {
        "id": "EgShdEnVNDIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in np.random.permutation(len(sentences)).copy()[:5]:\n",
        "    query_sentence = sentences[i]\n",
        "\n",
        "    query_vector = model.encode([query_sentence])\n",
        "    query_dot_vector = create_dot_vector(query_vector,plane_norms)\n",
        "    ids = get_candidate(query_dot_vector,2)\n",
        "    print('查询:', query_sentence)\n",
        "    for idx in ids:\n",
        "      print('   L2距离:',distance.euclidean(query_vector[0],sentence_embeddings[idx]),'匹配:',sentences[idx] )\n",
        "    # 测试faiss结果\n",
        "    D, I = indexLSH.search(query_vector, 2)\n",
        "    print('----Faiss----')\n",
        "    for idx in I[0]:\n",
        "      print('   L2距离:',distance.euclidean(query_vector[0],sentence_embeddings[idx]),'匹配:',sentences[idx] )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVeXRqbaAD7a",
        "outputId": "168e6bf4-3e5e-4c23-c41a-0f81a049a360"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "查询: 生产主管这职位好吗？我是刚毕业的大学生,应聘了一家大型外资企业的 \n",
            "   L2距离: 17.197383880615234 匹配: 鬼王高手进谢谢···一共150点技能点,按以下加法少侠重击　　　 \n",
            "   L2距离: 16.48116111755371 匹配: 午间指导（2007/10/31）今日大盘继续走好但股指呈现高开低 \n",
            "----Faiss----\n",
            "   L2距离: 16.745290756225586 匹配: 烦请各位大虾指点一下阵容，先谢过了后卫：理查兹，默特萨克，甘贝里 \n",
            "   L2距离: 17.793703079223633 匹配: 关于西安交大考研本人学金融的，但是对西方经济学不感兴趣,想考西安 \n",
            "查询: 古典中国风纯音乐古琴，琵琶，古筝之类的像女子十二乐坊的曲子 \n",
            "   L2距离: 20.520275115966797 匹配: 求一篇思想政治的论文要求:写一篇[关于大学生思想道德现状产生原因 \n",
            "   L2距离: 17.825180053710938 匹配: 她比我大2岁还跟过二个男友现在她选择了我她今年26她的前男友21 \n",
            "----Faiss----\n",
            "   L2距离: 0.0 匹配: 古典中国风纯音乐古琴，琵琶，古筝之类的像女子十二乐坊的曲子 \n",
            "   L2距离: 19.309078216552734 匹配: 做肉丸要用什么机器?是不是一台碎肉机及一台成型机就可以了呢?主要 \n",
            "查询: 西安人有买钻石投资的吗？我要在西安买钻石？西安有钻石买吗？哪家珠? \n",
            "   L2距离: 0.0 匹配: 西安人有买钻石投资的吗？我要在西安买钻石？西安有钻石买吗？哪家珠? \n",
            "   L2距离: 20.764333724975586 匹配: 进不了奇迹世界为什么总是提示文件升级失败然后就退出大区选择界面， \n",
            "----Faiss----\n",
            "   L2距离: 0.0 匹配: 西安人有买钻石投资的吗？我要在西安买钻石？西安有钻石买吗？哪家珠? \n",
            "   L2距离: 19.437467575073242 匹配: 世界历史人物世界上有那些国家的君主可以和秦皇汉武，唐宗宋祖相提并 \n",
            "查询: 请问金蛋编码怎么申请啊???金蛋编码要如何申请??怎么样才能领到 \n",
            "   L2距离: 0.0 匹配: 请问金蛋编码怎么申请啊???金蛋编码要如何申请??怎么样才能领到 \n",
            "   L2距离: 17.646564483642578 匹配: 我是新手请问大侠怎么可以投好3分 \n",
            "----Faiss----\n",
            "   L2距离: 19.34611701965332 匹配: 搏击中两人抱在一起是为什麽？经常看见拳击或自由搏击之中会有两个人 \n",
            "   L2距离: 18.23182487487793 匹配: 异地恋真得就那摸难吗???我该怎么来处理我这段异地的恋情??大家? \n",
            "查询: 大成优选可买否请高手告知大成优选是否配售，现可买否，谢谢同党! \n",
            "   L2距离: 18.105226516723633 匹配: 的鼻子很大,而且没有鼻梁,难看死了,有没有办法变小或者鼻梁变挺? \n",
            "   L2距离: 16.970487594604492 匹配: f(x)在x=a处可导，f(x)的绝对值在x=a处不可导的条件？ \n",
            "----Faiss----\n",
            "   L2距离: 17.72905921936035 匹配: 斯特冈上市很久了，有没有人用过斯特冈喷剂？ \n",
            "   L2距离: 19.630430221557617 匹配: 毛孔粗大怎么办我脸上长豆豆,出油,额头,鼻子上,脸上都有毛孔,很 \n"
          ]
        }
      ]
    }
  ]
}